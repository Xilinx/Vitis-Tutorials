


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

  
  
  
  

  
      <script type="text/javascript" src="../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/header-footer.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../_static/js/Searchbox.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pro.min.css" media="all" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="header parbase aem-GridColumn aem-GridColumn--default--12">
								<noindex>
									<header data-component="header">
										<nav class="navbar navbar-default aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid main-nav">
													<div class="row">
														<div class="col-xs-12">
															<div class="logo-column">
																<div class="logo">
																	<a href="https://www.xilinx.com/">
																	<img src="https://www.xilinx.com/etc.clientlibs/site/clientlibs/xilinx/all/resources/imgs/header/xilinx-header-logo.svg" title="Xilinx Inc"/>
																	</a>
																</div>
															</div>
															<div class="navbar-column">
																<div class="navbar navbar-collapse collapse" id="xilinx-main-menu">
																	<div class="mobile-search-container">
																		<div id="headerSearchBox" class="headerSearch"
																			data-component="header-search"
																			data-redirect-if-empty="false"
																			data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																			data-coveo-organization-id="xilinxcomprode2rjoqok">
																			<div class='coveo-search-section'>
																				<div class="CoveoAnalytics" data-search-hub="Site"></div>
																				<ul class="dropdown-menu options">
																					<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																						<a href="#">
																						All</a>
																					</li>
																					<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com//products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Silicon Devices</a>
																					</li>
																					<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com//products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Boards and Kits</a>
																					</li>
																					<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com//products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Intellectual Property</a>
																					</li>
																					<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																						<a href="#">
																						Support</a>
																						<ul>
																							<li data-label="Documentation" data-action-link="https://www.xilinx.com//support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																								<a href="#">
																								Documentation</a>
																							</li>
																							<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com//support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																								<a href="#">
																								Knowledge Base</a>
																							</li>
																							<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																								<a href="#">
																								Community Forums</a>
																							</li>
																						</ul>
																					</li>
																					<li data-label="Partners" data-action-link="https://www.xilinx.com//alliance/member-keyword-search.html" data-search-hub="Partner">
																						<a href="#">
																						Partners</a>
																					</li>
																					<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																						<a href="#">
																						Videos</a>
																					</li>
																					<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																						<a href="#">
																						Press</a>
																					</li>
																				</ul>
																				<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																				<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																			</div>
																		</div>
																	</div>
																	<ul class="nav navbar-nav nav-justified">
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/applications.html">
																			Applications</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/products/silicon-devices.html">
																			Products</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://developer.xilinx.com/">
																			Developers</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/support.html">
																			Support</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/about/company-overview.html">
																			About</a>
																		</li>
																	</ul>
																</div>
															</div>
															<script type="text/javascript" src="../../../_static/js/gtm.js"></script>
															<!--<div class="mini-nav">
																<button type="button" data-function="xilinx-mobile-menu" id="nav-toggle" class="navbar-toggle collapsed visible-xs-block" aria-expanded="false">
																<span></span>
																<span></span>
																<span></span>
																<span></span>
																</button>
																<ul class="list-inline">
																	<li class="dropdown user-menu">
																		<button data-toggle="dropdown">
																		<span class="sr-only">Account</span>
																		<span class="fas fa-user"></span>
																		</button>
																		<ul class="dropdown-menu">
																			<li>
																				<a href="https://www.xilinx.com/myprofile/subscriptions.html">
																				My Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/registration/create-account.html">
																				Create Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/bin/protected/en/signout">
																				Sign Out</a>
																			</li>
																		</ul>
																	</li>
																	<li class="hidden-xs">
																		<button data-function="search-toggle">
																		<span class="sr-only">Search</span>
																		<span class="far fa-search"></span>
																		</button>
																	</li>
																</ul>
															</div>
															-->
															<div class="search-container">
																<div id="headerSearchBox" class="headerSearch"
																	data-component="header-search"
																	data-redirect-if-empty="false"
																	data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																	data-coveo-organization-id="xilinxcomprode2rjoqok">
																	<div class='coveo-search-section'>
																		<div class="CoveoAnalytics" data-search-hub="Site"></div>
																		<ul class="dropdown-menu options">
																			<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																				<a href="#">
																				All</a>
																			</li>
																			<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com/products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Silicon Devices</a>
																			</li>
																			<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com/products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Boards and Kits</a>
																			</li>
																			<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com/products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Intellectual Property</a>
																			</li>
																			<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																				<a href="#">
																				Support</a>
																				<ul>
																					<li data-label="Documentation" data-action-link="https://www.xilinx.com/support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																						<a href="#">
																						Documentation</a>
																					</li>
																					<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com/support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																						<a href="#">
																						Knowledge Base</a>
																					</li>
																					<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																						<a href="#">
																						Community Forums</a>
																					</li>
																				</ul>
																			</li>
																			<li data-label="Partners" data-action-link="https://www.xilinx.com/alliance/member-keyword-search.html" data-search-hub="Partner">
																				<a href="#">
																				Partners</a>
																			</li>
																			<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																				<a href="#">
																				Videos</a>
																			</li>
																			<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																				<a href="#">
																				Press</a>
																			</li>
																		</ul>
																		<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																		<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																	</div>
																</div>
																<button data-function="search-toggle">
																<span class="sr-only">Search</span>
																<span class="far fa-times"></span>
																</button>
															</div>
														</div>
													</div>
												</div>
											</div>
										</nav>
									</header>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis In-Depth Tutorials
          

          
          </a>

          
            
            
              <div class="version">
                2020.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption"><span class="caption-text">日本語版</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs-jp/README.html">Master</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis/README.html">Vitis Flow 101 Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis_HLS/README.html">Vitis HLS Analysis and Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Introduction to Machine Learning with Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Acceleration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html">Introduction to Vitis Hardware Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">AI Engine Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html">AI Engine Development</a></li>
</ul>
<p class="caption"><span class="caption-text">Platform Creation Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Vitis_Platform_Creation/README.html">Platform Creation</a></li>
</ul>
<p class="caption"><span class="caption-text">XRT and Vitis System Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html">Xilinx Runtime (XRT) and Vitis System Optimization Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/blob/Vitis-Tutorials-2019.2-Hotfix1/README.md">2019.2</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../_sources/Machine_Learning/Design_Tutorials/04-Keras_GoogleNet_ResNet/README.md.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis In-Depth Tutorials</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Current status</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Machine_Learning/Design_Tutorials/04-Keras_GoogleNet_ResNet/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div style="page-break-after: always;"></div>
<table style="width:100%">
  <tr>
    <th width="100%" colspan="6"><img src="https://www.xilinx.com/content/dam/xilinx/imgs/press/media-kits/corporate/xilinx-logo.png" width="30%"/><h1>Deep Learning with Custom GoogleNet and ResNet in Keras and Xilinx Vitis AI</h1>
</th>
  </tr>
</table>
</div><div class="section" id="current-status">
<h1>Current status<a class="headerlink" href="#current-status" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p>Tested with Vitis AI 1.3</p></li>
<li><p>Tested in hardware on ZCU102 (all four CNNs)</p></li>
<li><p>Tested in hardware on VCK190 (missing miniResNet CNN)</p></li>
</ol>
<p><strong>Date: 9 Jan 2021</strong></p>
</div>
<div class="section" id="introduction">
<h1>1 Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>In this Deep Learning (DL) tutorial, you will quantize in fixed point some custom Convolutional Neural Networks (CNNs) and deploy them on the Xilinx® <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a>, <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/zcu104.html">ZCU104</a> and <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/vck190.html">VCK190</a> boards using <a class="reference external" href="https://developer.xilinx.com/en/get-started/ai.html">Vitis AI</a>, which is a set of optimized IP, tools libraries, models and example designs valid for AI inference on both Xilinx edge devices and Alveo cards.</p>
<p>This tutorial includes:</p>
<ul class="simple">
<li><p>four custom CNNs, from the simplest <code class="docutils literal notranslate"><span class="pre">LeNet</span></code> and <code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code> to the intermediate <code class="docutils literal notranslate"><span class="pre">miniGoogleNet</span></code> and the more complex <code class="docutils literal notranslate"><span class="pre">miniResNet</span></code>, as described in the <a class="reference external" href="files/code/custom_cnn.py">custom_cnn.py</a> file;</p></li>
<li><p>two different datasets, <code class="docutils literal notranslate"><span class="pre">Fashion-MNIST</span></code> and <code class="docutils literal notranslate"><span class="pre">CIFAR-10</span></code>, each one with 10 classes of objects.</p></li>
</ul>
<p>Once the selected CNN has been correctly trained in Keras, the <a class="reference external" href="https://www.hdfgroup.org/solutions/hdf5/">HDF5</a> file of weights is converted into a TF checkpoint and inference graph file, such frozen graph is then quantized by the Vitis AI Quantizer that creates an INT8 <code class="docutils literal notranslate"><span class="pre">pb</span></code> file from which the Vitis AI Compiler generates the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file of micro instructions for the Deep Processor Unit (DPU) of the Vitis AI platform. The final C++ application is executed at run time on the ZCU102 target board, which is the default one adopted in this tutorial, see the Appendix for the ZCU104 and VCK190. The top-1 accuracy of the predictions computed at run time is measured and compared with the simulation results.</p>
</div>
<div class="section" id="prerequisites">
<h1>2 Prerequisites (<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Ubuntu 16.04 host PC with Python 3.6.</p></li>
<li><p>The entire repository of <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.3</a> from <a class="reference external" href="https://www.github.com/Xilinx">www.github.com/Xilinx</a>.</p></li>
<li><p>Accurate reading of <a class="reference external" href="https://www.xilinx.com/support/documentation/sw_manuals/vitis_ai/1_3/ug1414-vitis-ai.pdf">Vitis AI User Guide UG1414 v1.3</a>. In particular:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="s2">&quot;Vitis AI Overview&quot;</span> <span class="ow">in</span> <span class="n">Chapter</span> <span class="mi">1</span> <span class="k">with</span> <span class="n">DPU</span> <span class="n">naming</span> <span class="ow">and</span> <span class="n">guidelines</span> <span class="n">to</span> <span class="n">download</span> <span class="n">the</span> <span class="n">tools</span> <span class="n">container</span> <span class="n">available</span> <span class="kn">from</span> <span class="p">[</span><span class="n">docker</span> <span class="n">hub</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">hub</span><span class="o">.</span><span class="n">docker</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">r</span><span class="o">/</span><span class="n">xilinx</span><span class="o">/</span><span class="n">vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">/</span><span class="n">tags</span><span class="p">)</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">Runtime</span> <span class="n">Package</span> <span class="k">for</span> <span class="n">edge</span> <span class="p">(</span><span class="n">MPSoC</span><span class="p">)</span> <span class="n">devices</span><span class="o">.</span>
<span class="mf">2.</span> <span class="s2">&quot;Installation and Setup&quot;</span> <span class="n">instructions</span> <span class="n">of</span> <span class="n">Chapter</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">both</span> <span class="n">host</span> <span class="ow">and</span> <span class="n">target</span><span class="p">;</span>
<span class="mf">3.</span> <span class="s2">&quot;Quantizing the Model&quot;</span> <span class="ow">in</span> <span class="n">Chapter</span> <span class="mi">4</span> <span class="ow">and</span> <span class="s2">&quot;Compiling the Model&quot;</span> <span class="ow">in</span> <span class="n">Chapter</span> <span class="mf">5.</span>
<span class="mf">4.</span> <span class="s2">&quot;Programming with VART&quot;</span> <span class="n">APIs</span> <span class="ow">in</span> <span class="n">Chapter</span> <span class="mf">6.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A Vitis AI Evaluation board such as either:</p>
<ul>
<li><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">ZCU102</a> with its <a class="reference external" href="https://www.xilinx.com/bin/public/openDownload?filename=xilinx-zcu102-dpu-v2020.2-r1.3.0.2.0.img.gz">image file</a>, which contains a pre-built working design for the ZCU102 with the DPUCZDX8G (renamed shortly as “DPUv2” in the following), or</p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/vck190.html">VCK190</a> with its <a class="reference external" href="#">image file</a>, which contains a pre-built working design for the VCK190 with the DPUCVDX8G (renamed shortly as “XVDPU”).</p></li>
</ul>
</li>
<li><p>Familiarity with Deep Learning principles.</p></li>
</ul>
<div class="section" id="dos-to-unix-conversion">
<h2>Dos-to-Unix Conversion<a class="headerlink" href="#dos-to-unix-conversion" title="Permalink to this headline">¶</a></h2>
<p>In case you might get some strange errors during the execution of the scripts, you have to pre-process -just once- all the<code class="docutils literal notranslate"><span class="pre">*.sh</span></code> shell and the python <code class="docutils literal notranslate"><span class="pre">*.py</span></code> scripts with the <a class="reference external" href="http://archive.ubuntu.com/ubuntu/pool/universe/d/dos2unix/dos2unix_6.0.4.orig.tar.gz">dos2unix</a> utility.
In that case run the following commands from your Ubuntu host PC (out of the Vitis AI docker images):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt-get install dos2unix
<span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1">#your working directory</span>
<span class="k">for</span> file in <span class="k">$(</span>find . -name <span class="s2">&quot;*.sh&quot;</span><span class="k">)</span><span class="p">;</span> <span class="k">do</span>
  dos2unix <span class="si">${</span><span class="nv">file</span><span class="si">}</span>
<span class="k">done</span>
</pre></div>
</div>
</div>
<div class="section" id="vitis-ai-1-2">
<h2>Vitis AI 1.2<a class="headerlink" href="#vitis-ai-1-2" title="Permalink to this headline">¶</a></h2>
<p>If you need to use the older Vitis AI 1.2 release, just replace this <code class="docutils literal notranslate"><span class="pre">README.md</span></code> file with the one placed in the subfolder
<code class="docutils literal notranslate"><span class="pre">vai_1v2</span></code> and go on in following the instructions on that file and the related <code class="docutils literal notranslate"><span class="pre">vai_1v2.zip</span></code> archive, then skip the rest of this document.</p>
</div>
</div>
<div class="section" id="before-starting-with-vitis-ai-1-3">
<h1>3 Before starting with Vitis AI 1.3<a class="headerlink" href="#before-starting-with-vitis-ai-1-3" title="Permalink to this headline">¶</a></h1>
<p>In the following of this document, it is assumed that you have cloned the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis AI stack release 1.2</a> and this is your working directory <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> (for example in my case I renamed it as <code class="docutils literal notranslate"><span class="pre">~/ML/VAI1v3</span></code>).</p>
<p>To list the currently available docker images run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker images <span class="c1"># to list the current docker images available in the host pc</span>
</pre></div>
</div>
<p>and you should see something like in the following text:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>REPOSITORY            TAG                               IMAGE ID            CREATED             SIZE
xilinx/vitis-ai-gpu   latest                            1bc243fc037a        41 minutes ago      19GB
</pre></div>
</div>
<p>To launch the docker container with Vitis AI tools - to do all the steps from CNN training to generation of the ELF file for the DPU - based on CPU (or GPU), execute the following commands from the <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt; <span class="c1"># you are now in Vitis_AI subfolder</span>
./docker_run.sh xilinx/vitis-ai-gpu:1.3
conda activate vitis-ai-tensorflow
</pre></div>
</div>
<p>Note that the container maps the shared folder <code class="docutils literal notranslate"><span class="pre">/workspace</span></code> with the file system of the Host PC from where you launch the above command, which is <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;</span></code> in your case.
This shared folder enables you to transfer files from the Host PC to the docker container and vice versa.</p>
<p>The docker container do not have any graphic editor, so it is recommended that you work with two terminals and you point to the same folder, in one terminal you use the docker container commands and in the other terminal you open any graphic editor you like.</p>
<p>Note that docker does not have an automatic garbage collection system as of now. You can use this command to do a manual garbage collection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker rmi -f $(docker images -f &quot;dangling=true&quot; -q)
</pre></div>
</div>
<div class="section" id="install-missing-packages-on-the-vitis-ai-tools-container">
<h2>3.1 Install Missing Packages on the Vitis AI Tools Container<a class="headerlink" href="#install-missing-packages-on-the-vitis-ai-tools-container" title="Permalink to this headline">¶</a></h2>
<p>This tutorial requires some packages that were not included in the original Vitis AI tools container. Here are the commands to include such packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt;
./docker_run.sh xilinx/vitis-ai-gpu:1.3     
sudo su <span class="c1"># you must be root</span>
conda activate vitis-ai-tensorflow <span class="c1"># as root, enter into Vitis AI TF (anaconda-based) virtual environment</span>
conda install seaborn <span class="c1"># THIS ARE STILL NEEDED IN VAI 1.3 DOCKER TOOLS IMAGE</span>
conda install <span class="nv">pycairo</span><span class="o">==</span><span class="m">1</span>.18.2 <span class="c1">#not sure if needed by Vitis AI 1.3</span>
<span class="c1"># you cannot install next packages with conda, so use pip instead</span>
pip install <span class="nv">imutils</span><span class="o">==</span><span class="m">0</span>.5.1 <span class="c1">#not sure if needed by Vitis AI 1.3</span>
conda deactivate
<span class="nb">exit</span> <span class="c1"># to exit from root</span>
conda activate vitis-ai-tensorflow <span class="c1"># as normal user, enter into Vitis AI TF (anaconda-based) virtual environment</span>
</pre></div>
</div>
<p>Note that if you exit from the current Docker Vitis AI tools image you will lose all the installed packages, so to save all changes in a new docker image open a new terminal and run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker ps -l <span class="c1"># To get the Docker CONTAINER ID</span>
</pre></div>
</div>
<p>you will see the following text (the container ID might have a different number):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>CONTAINER ID        IMAGE                        COMMAND                CREATED             STATUS              NAMES
7c9927375b06        xilinx/vitis-ai-gpu:1.3   &quot;/etc/login.sh bash&quot;   30 minutes ago      Up 30 minutes       heuristic_lamport
</pre></div>
</div>
<p>now save the modified docker image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo docker commit -m<span class="s2">&quot;comment&quot;</span> 7c9927375b06 xilinx/vitis-ai-gpu:latest
</pre></div>
</div>
<p>Assuming you have renamed this project <code class="docutils literal notranslate"><span class="pre">VAI-Keras-GoogleNet-ResNet</span></code> and placed it in the directory named <code class="docutils literal notranslate"><span class="pre">&lt;WRK_DIR&gt;/tutorials/</span></code>, you can launch the modified tools container by running the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> &lt;WRK_DIR&gt;
./docker_run.sh xilinx/vitis-ai-gpu:1.3
conda activate vitis-ai-tensorflow
<span class="nb">cd</span> /workspace/tutorials/VAI-Keras-GoogleNet-ResNet
</pre></div>
</div>
</div>
</div>
<div class="section" id="the-main-flow">
<h1>4 The Main Flow<a class="headerlink" href="#the-main-flow" title="Permalink to this headline">¶</a></h1>
<p>The main flow is composed of seven major steps. The first six steps are executed from the tools container on the host PC by launching the script <a class="reference external" href="files/run_all.sh">run_all.sh</a>, which contains several functions. The seventh step can be executed directly on the target board.
Here is an overview of each step.</p>
<ol class="simple">
<li><p>Organize the data into folders, such as <code class="docutils literal notranslate"><span class="pre">train</span></code> for training, <code class="docutils literal notranslate"><span class="pre">val</span></code> for validation during the training phase, <code class="docutils literal notranslate"><span class="pre">test</span></code> for testing during the inference/prediction phase, and <code class="docutils literal notranslate"><span class="pre">cal</span></code> for calibration during the quantization phase, for each dataset. See <a class="reference external" href="#41-organize-the-data">Organize the Data</a> for more information.</p></li>
<li><p>Train the CNNs in Keras and generate the HDF5 weights model. See <a class="reference external" href="#42-train-the-cnn">Train the CNN</a> for more information.</p></li>
<li><p>Convert into TF checkpoints and inference graphs. See <a class="reference external" href="#43-create-tf-inference-graphs-from-keras-models">Create TF Inference Graphs from Keras Models</a> for more information.</p></li>
<li><p>Freeze the TF graphs to evaluate the CNN prediction accuracy as the reference starting point. See <a class="reference external" href="#44-freeze-the-tf-graphs">Freeze the TF Graphs</a> for more information.</p></li>
<li><p>Quantize from 32-bit floating point to 8-bit fixed point and evaluate the prediction accuracy of the quantized CNN. See <a class="reference external" href="#45-quantize-the-frozen-graphs">Quantize the Frozen Graphs</a> for more information.</p></li>
<li><p>Run the compiler to generate the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file for the target board From the quantized <code class="docutils literal notranslate"><span class="pre">pb</span></code> file. See <a class="reference external" href="#46-compile-the-quantized-models">Compile the Quantized Models</a> for more information.</p></li>
<li><p>Use either VART C++ or Python APIs to write the hybrid application for the ARM CPU, then cross-compile it in the <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> host environment.  The application is called “hybrid” because the ARM CPU is executing some software routines while the DPU hardware accelerator is running the FC, CONV, ReLU, and BN layers of the CNN that were coded in the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code>file.</p></li>
<li><p>Assuming you have archived the <code class="docutils literal notranslate"><span class="pre">target_zcu102</span></code> folder and transferred the related <code class="docutils literal notranslate"><span class="pre">target_zcu102.tar</span></code> archive from the host to the target board with <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, now you can run the hybrid application. See <a class="reference external" href="#47-build-and-run-on-the-zcu102-target-board">Build and Run on the ZCU102 Target Board</a> for more information.</p></li>
</ol>
<blockquote>
<div><p><strong>:pushpin: NOTE</strong> All explanations in the following sections are based only on the Fashion-MNIST dataset; the commands for the CIFAR-10 dataset are very similar: just replace the sub-string “fmnist” with “cifar10”.</p>
</div></blockquote>
<div class="section" id="organize-the-data">
<h2>4.1 Organize the Data<a class="headerlink" href="#organize-the-data" title="Permalink to this headline">¶</a></h2>
<p>As Deep Learning deals with image data, you have to organize your data in appropriate folders and apply some pre-processing to adapt the images to  the hardware features of the Vitis AI Platform. The first lines of script <a class="reference external" href="files/run_all.sh">run_all.sh</a> call other python scripts to create the sub-folders <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">val</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, and <code class="docutils literal notranslate"><span class="pre">cal</span></code> that are located in the <code class="docutils literal notranslate"><span class="pre">dataset/fashion-mnist</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset/cifar10</span></code> directories and to fill them with 50000 images for training, 5000 images for validation, 5000 images for testing (taken from the 10000 images of the original test dataset) and 1000 images for the calibration process (copied from the training images).</p>
<p>All the images are 32x32x3 in dimensions so that they are compatible with the two different datasets.</p>
<div class="section" id="fashion-mnist">
<h3>4.1.1 Fashion MNIST<a class="headerlink" href="#fashion-mnist" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> dataset is considered the <code class="docutils literal notranslate"><span class="pre">hello</span> <span class="pre">world</span></code> of DL because it is widely used as a first test to check the deployment flow of a vendor of DL solutions. This small dataset takes relatively less time in the training of any CNN. However, due to the poor content of all its images, even the most shallow CNN can easily achieve from 98% to 99% of top-1 accuracy in Image Classification.</p>
<p>To solve this problem, the <a class="reference external" href="https://github.com/zalandoresearch/fashion-mnist">Fashion-MNIST</a> dataset has been recently created for the paper <a class="reference external" href="arxiv.org/abs/1708.07747">Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms</a>. It is identical to the MNIST dataset in terms of training set size, testing set size, number of class labels, and image dimensions, but it is more challenging in terms of achieving high top-1 accuracy values.</p>
<p>Usually, the size of the images is 28x28x1 (gray-level), but in this case they have been converted to 32x32x3 (“false” RGB images) to be compatible with the “true” RGB format of CIFAR-10.</p>
</div>
<div class="section" id="cifar-10">
<h3>4.1.2 CIFAR-10<a class="headerlink" href="#cifar-10" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a> dataset is composed of 10 classes of objects to be classified. It contains 60000 labeled RGB images that are 32x32 in size and thus, this dataset is more challenging than the MNIST and Fashion-MNIST datasets. The CIFAR-10 dataset was developed for the paper <a class="reference external" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Learning Multiple Layers of Features from Tiny Images</a>.</p>
</div>
</div>
<div class="section" id="train-the-cnn">
<h2>4.2 Train the CNN<a class="headerlink" href="#train-the-cnn" title="Permalink to this headline">¶</a></h2>
<p>Irrespective of the CNN type, the data is processed, using the following Python code, to normalize it from 0 to 1. Such code has to be mirrored in the C++ application that runs in the ARM® CPU of ZCU102 target board.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># scale data to the range of [0, 1]</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cfg</span><span class="o">.</span><span class="n">NORM_FACTOR</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="n">cfg</span><span class="o">.</span><span class="n">NORM_FACTOR</span>

<span class="c1"># normalize</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">-</span><span class="mf">0.5</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span> <span class="o">*</span><span class="mi">2</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span>  <span class="o">-</span><span class="mf">0.5</span>
<span class="n">x_test</span>  <span class="o">=</span> <span class="n">x_test</span>  <span class="o">*</span><span class="mi">2</span>
</pre></div>
</div>
<div class="section" id="lenet">
<h3>4.2.1 LeNet<a class="headerlink" href="#lenet" title="Permalink to this headline">¶</a></h3>
<p>The model scheme of <code class="docutils literal notranslate"><span class="pre">LeNet</span></code> has 6,409,510 parameters as shown in the following figure:</p>
<p><img alt="figure" src="../../../_images/bd_LeNet.png" /></p>
<p>Once the training is complete, you will get the average top-1 accuracy as reported in the logfile placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder.</p>
<p>For more details about this custom CNN and its training procedure, read the “Starter Bundle” of the <a class="reference external" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for Computer Vision with Python</a> books by Dr. Adrian Rosebrock.</p>
</div>
<div class="section" id="minivggnet">
<h3>4.2.2 miniVggNet<a class="headerlink" href="#minivggnet" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code> is a less deep version of the original <code class="docutils literal notranslate"><span class="pre">VGG16</span></code> CNN customized for the smaller Fashion-MNIST dataset instead of the larger <a class="reference external" href="https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/">ImageNet-based ILSVRC</a>. For more information on this custom CNN and its training procedure, read <a class="reference external" href="https://www.pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/">Adrian Rosebrock’s post</a> from the PyImageSearch Keras Tutorials. <code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code> is also explained in the “Practitioner Bundle” of the <a class="reference external" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for CV with Python</a> books.</p>
<p>The model scheme of <code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code> has 2,170,986 parameters as shown in the following figure:</p>
<p><img alt="figure" src="../../../_images/bd_miniVggNet.png" /></p>
<p>Once the training is complete, you will get the average top-1 accuracy as reported in the logfile placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder and also illustrated by the learning curves:</p>
<p><img alt="figure" src="../../../_images/miniVggNet_plot.png" /></p>
</div>
<div class="section" id="minigooglenet">
<h3>4.2.3 miniGoogleNet<a class="headerlink" href="#minigooglenet" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">miniGoogleNet</span></code> is a customization of the original <code class="docutils literal notranslate"><span class="pre">GoogleNet</span></code> CNN. It is suitable for the smaller Fashion-MNIST dataset, instead of the larger ImageNet-based ILSVRC.</p>
<p>For more information on <code class="docutils literal notranslate"><span class="pre">miniGoogleNet</span></code>, read the “Practitioner Bundle” of the <a class="reference external" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for CV with Python</a> books by Dr. Adrian Rosebrock.</p>
<p>The model scheme of <code class="docutils literal notranslate"><span class="pre">miniGoogleNet</span></code> has 1,656,250 parameters, as shown in the following figure:</p>
<p><img alt="figure" src="../../../_images/bd_miniGoogleNet.png" /></p>
<p>Once the training is complete, you will get the average top-1 accuracy as reported  in the logfile placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder and also illustrated by the learning curves:</p>
<p><img alt="figure" src="../../../_images/miniGoogleNet_plot.png" /></p>
</div>
<div class="section" id="miniresnet">
<h3>4.2.4 miniResNet<a class="headerlink" href="#miniresnet" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">miniResNet</span></code> is a customization of the original <code class="docutils literal notranslate"><span class="pre">ResNet-50</span></code> CNN. It is suitable for the smaller Fashion-MNIST small dataset, instead of the larger ImageNet-based ILSVRC.</p>
<p>For more information on <code class="docutils literal notranslate"><span class="pre">miniResNet</span></code>, read the “Practitioner Bundle” of the <a class="reference external" href="https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/">Deep Learning for CV with Python</a> books.</p>
<p>The model scheme of <code class="docutils literal notranslate"><span class="pre">miniResNet</span></code> has  886,102 parameters, as shown in the following figure:</p>
<p><img alt="figure" src="../../../_images/bd_miniResNet.png" /></p>
<p>Once the training is complete, you will get the average top-1 accuracy as reported
in the  in the logfile placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder and also reported by the learning curves:</p>
<p><img alt="figure" src="../../../_images/miniResNet_plot.png" /></p>
</div>
</div>
<div class="section" id="create-tf-inference-graphs-from-keras-models">
<h2>4.3 Create TF Inference Graphs from Keras Models<a class="headerlink" href="#create-tf-inference-graphs-from-keras-models" title="Permalink to this headline">¶</a></h2>
<p>The function <code class="docutils literal notranslate"><span class="pre">2_fmnist_Keras2TF()</span></code> gets the computation graph of the TF backend representing the Keras model which includes the forward pass and training related operations.</p>
<p>The output files of this process, <code class="docutils literal notranslate"><span class="pre">infer_graph.pb</span></code> and <code class="docutils literal notranslate"><span class="pre">float_model.chkpt.*</span></code>, will be stored in the folder <a class="reference external" href="files/tf_chkpts/">tf_chkpts</a> (actually empty to save disk space). The generated logfile in the <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder also contains the TF input and output names that will be needed for <a class="reference external" href="#freeze-the-tf-graphs">Freeze the TF Graphs</a>, for example, in the case of <code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code>, such nodes are named <code class="docutils literal notranslate"><span class="pre">conv2d_1_input</span></code> and <code class="docutils literal notranslate"><span class="pre">activation_6/Softmax</span></code> respectively.</p>
</div>
<div class="section" id="freeze-the-tf-graphs">
<h2>4.4 Freeze the TF Graphs<a class="headerlink" href="#freeze-the-tf-graphs" title="Permalink to this headline">¶</a></h2>
<p>The inference graph created in <a class="reference external" href="#create-tf-inference-graphs-from-keras-models">Create TF Inference Graphs from Keras Models</a> is first converted to a <a class="reference external" href="https://www.tensorflow.org/guide/extend/model_files">GraphDef protocol buffer</a>, then cleaned so that the subgraphs that are not necessary to compute the requested outputs, such as the training operations, can be removed. This process is called “freezing the graph”.</p>
<p>The routines  <code class="docutils literal notranslate"><span class="pre">3a_fmnist_freeze()</span></code> and <code class="docutils literal notranslate"><span class="pre">3b_fmnist_evaluate_frozen_graph()</span></code> generate the frozen graph and use it to evaluate the accuracy of the CNN by making predictions on the images in the <code class="docutils literal notranslate"><span class="pre">test</span></code> folder.</p>
<p>It is important to apply the correct <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">node</span></code> and <code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">node</span></code> names in all the shell scripts, as shown in the following example with parameters when related to the <code class="docutils literal notranslate"><span class="pre">miniVggNet</span></code> case study:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">input_node</span>  <span class="n">conv2d_1_input</span> <span class="o">--</span><span class="n">output_node</span> <span class="n">activation_6</span><span class="o">/</span><span class="n">Softmax</span>
</pre></div>
</div>
<p>This information can be captured by the following python code:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the input and output name</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> TF input node name:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> TF output node name:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>The frozen graphs evaluation generates top-1 prediction accuracy as reported in the log files placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder.</p>
</div>
<div class="section" id="quantize-the-frozen-graphs">
<h2>4.5 Quantize the Frozen Graphs<a class="headerlink" href="#quantize-the-frozen-graphs" title="Permalink to this headline">¶</a></h2>
<p>The routines <code class="docutils literal notranslate"><span class="pre">4a_fmnist_quant()</span></code> and <code class="docutils literal notranslate"><span class="pre">4b_fmnist_evaluate_quantized_graph()</span></code>
generate the quantized graph and use it to evaluate the accuracy of the CNN by making predictions on the images in the <code class="docutils literal notranslate"><span class="pre">test</span></code> folder.</p>
<p>The quantized graphs evaluation generates top-1 prediction accuracy as reported in the log files placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder.</p>
</div>
<div class="section" id="compile-the-quantized-models">
<h2>4.6 Compile the Quantized Models<a class="headerlink" href="#compile-the-quantized-models" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">5_fmnist_vai_compile_zcu102()</span></code> routine generates the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file for the embedded system composed by the ARM CPU and the DPU accelerator in the ZCU102 board.</p>
<p>This file has to be loaded at run time from the C++ (or Python) application directly on the target board OS environment. For example, in case of <code class="docutils literal notranslate"><span class="pre">LeNet</span></code> for Fashion-MNIST, the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> file is named <code class="docutils literal notranslate"><span class="pre">LeNet.xmodel</span></code>. A similar nomenclature is applied for the other CNNs.</p>
<p>Note that the Vitis AI Compiler tells you the names of the input and output nodes of the CNN that will be effectively implemented as a kernel in the DPU, therefore whatever layer remains out of such nodes it has to be executed in the ARM CPU as a software kernel.
This can be easily understood looking at the logfile of this step, for example case of <code class="docutils literal notranslate"><span class="pre">LeNet</span></code> CNN:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Input Node(s)             (H*W*C)
conv2d_2_convolution(0) : 32*32*3

Output Node(s)      (H*W*C)
dense_2_MatMul(0) : 1*1*10
</pre></div>
</div>
</div>
<div class="section" id="build-and-run-on-zcu102-target-board">
<h2>4.7 Build and Run on ZCU102 Target Board<a class="headerlink" href="#build-and-run-on-zcu102-target-board" title="Permalink to this headline">¶</a></h2>
<p>This section reports only the results related to Fashion-MNIST dataset. The results for CIFAR-10 are similar.</p>
<p>You have to cross-compile the hybrid (CPU + DPU) application from the host side (out of the docker tools image) with <a class="reference external" href="files/target_zcu102/code/build_app.sh">build_app.sh</a> shell script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span> LD_LIBRARY_PATH   
sh ~/petalinux_sdk/2020.2/environment-setup-aarch64-xilinx-linux <span class="c1"># set petalinux environment of Vitis AI 1.1</span>
<span class="nb">cd</span> &lt;WRK_DIR&gt;/tutorials/VAI-Keras-GoogleNet-ResNet/files
<span class="nb">cd</span> target_zcu102/code
bash -x ./build_app.sh
mv code ../run_cnn <span class="c1"># change name of the application</span>
<span class="nb">cd</span> ..  
tar -cvf target_zcu102.tar ./target_zcu102 <span class="c1"># to be copied on the SD card</span>
</pre></div>
</div>
<p>Note that a subset of the <code class="docutils literal notranslate"><span class="pre">petalinux_sdk</span></code> environment is also available directly on the SD card target board, so you can compile the application directly from there. In fact this is what the script <code class="docutils literal notranslate"><span class="pre">run_all_fmnist_target.sh</span></code> indeed does, once you will launch it from the target board.</p>
<p>Assuming you have transferred the <code class="docutils literal notranslate"><span class="pre">target_zcu102.tar</span></code> archive from the host to the target board with the <code class="docutils literal notranslate"><span class="pre">scp</span></code> utility, you can now run the following command directly on the target board:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar -xvf target_zcu102.tar
<span class="nb">cd</span> target_zcu102
bash ./run_all_target.sh
</pre></div>
</div>
<div class="section" id="the-c-application-with-vart-apis">
<h3>4.7.1 The C++ Application with VART APIs<a class="headerlink" href="#the-c-application-with-vart-apis" title="Permalink to this headline">¶</a></h3>
<p>The C++ code for image classification <a class="reference external" href="files/target_zcu102/code/src/main.cc">main.cc</a> is independent of the CNN type, thanks to the abstraction done by the VART APIs.</p>
<p>It is very important that the C++ code for pre-processing the images executes the same operations that you applied in the Python code of the training procedure. This is illustrated in the following C++ code fragments:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cm">/*image pre-process*/</span>
<span class="n">Mat</span> <span class="n">image2</span> <span class="o">=</span> <span class="n">cv</span><span class="o">::</span><span class="n">Mat</span><span class="p">(</span><span class="n">inHeight</span><span class="p">,</span> <span class="n">inWidth</span><span class="p">,</span> <span class="n">CV_8SC3</span><span class="p">);</span>
<span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">image2</span><span class="p">,</span> <span class="n">Size</span><span class="p">(</span><span class="n">inHeight</span><span class="p">,</span> <span class="n">inWidth</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">INTER_NEAREST</span><span class="p">);</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">h</span> <span class="o">&lt;</span> <span class="n">inHeight</span><span class="p">;</span> <span class="n">h</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">w</span> <span class="o">&lt;</span> <span class="n">inWidth</span><span class="p">;</span> <span class="n">w</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">imageInputs</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">inSize</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">inWidth</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">c</span> <span class="p">]</span> <span class="o">=</span> <span class="p">(</span> <span class="kt">float</span><span class="p">(</span><span class="n">image2</span><span class="p">.</span><span class="n">at</span><span class="o">&lt;</span><span class="n">Vec3b</span><span class="o">&gt;</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)[</span><span class="n">c</span><span class="p">])</span><span class="o">/</span><span class="mf">255.0f</span> <span class="o">-</span> <span class="mf">0.5f</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">;</span> <span class="c1">//if you use BGR</span>
    <span class="c1">//imageInputs[i * inSize + h * inWidth * 3 + w * 3 +2-c] = ( float(image2.at&lt;Vec3b&gt;(h, w)[c])/255.0f - 0.5f )*2; //if you use RGB  </span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<blockquote>
<div><p><strong>:pushpin: NOTE</strong> The DPU API apply <a class="reference external" href="https://opencv.org/">OpenCV</a> functions to read an image file (either <code class="docutils literal notranslate"><span class="pre">png</span></code> or <code class="docutils literal notranslate"><span class="pre">jpg</span></code> or whatever format) therefore the images are seen as BGR and not as native RGB. All the training and inference steps done in this tutorial threats images as BGR, which is true also for the above C++ normalization routine.
A mismatch at this level would prevent the computation of the correct predictions at run time on the target board.</p>
</div></blockquote>
</div>
<div class="section" id="running-the-four-cnns">
<h3>4.7.2 Running the four CNNs<a class="headerlink" href="#running-the-four-cnns" title="Permalink to this headline">¶</a></h3>
<p>Turn on your target board and establish a serial communication with a <code class="docutils literal notranslate"><span class="pre">putty</span></code> terminal from Ubuntu or with a <code class="docutils literal notranslate"><span class="pre">TeraTerm</span></code> terminal from your Windows host PC.</p>
<p>Ensure that you have an Ethernet point-to-point cable connection with the correct IP addresses to enable <code class="docutils literal notranslate"><span class="pre">ssh</span></code> communication in order to quickly transfer files to the target board with <code class="docutils literal notranslate"><span class="pre">scp</span></code> from Ubuntu or <code class="docutils literal notranslate"><span class="pre">pscp.exe</span></code> from Windows host PC. For example, you can set the IP addresses of the target board to be <code class="docutils literal notranslate"><span class="pre">192.168.1.100</span></code> while the host PC is  <code class="docutils literal notranslate"><span class="pre">192.168.1.101</span></code> as shown in the following figure:</p>
<p><img alt="figure" src="../../../_images/teraterm.png" /></p>
<p>Once a <code class="docutils literal notranslate"><span class="pre">tar</span></code> file of the <a class="reference external" href="files/target_zcu102">target_zcu102</a> folder has been created, copy it from the host PC to the target board. For example, in case of an Ubuntu PC, use the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">target_zcu102</span><span class="o">.</span><span class="n">tar</span> <span class="n">root</span><span class="o">@</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.100</span><span class="p">:</span><span class="o">~/</span>
</pre></div>
</div>
<p>From the target board terminal, run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tar -xvf target_zcu102.tar
<span class="nb">cd</span> target_zcu102
bash -x ./run_all_fmnist_target.sh
bash -x ./run_all_cifar10_target.sh
</pre></div>
</div>
<p>With this command, the <code class="docutils literal notranslate"><span class="pre">fmnist_test.tar</span></code> file with the 5000 test images will be uncompressed.
The single-thread application based on VART C++ APIs is built with the <code class="docutils literal notranslate"><span class="pre">build_app.sh</span></code> script and finally launched for each CNN, the effective top-5 classification accuracy is checked by a python script like <a class="reference external" href="files/target_zcu102/code/src/check_runtime_top5_fmnist.py">check_runtime_top5_fmnist.py</a>.</p>
<p>Another script like <a class="reference external" href="files/target_zcu102/code/fps_fmnist.sh">fps_fmnist.sh</a> launches the multi-thread application based on VART Python APIs to measure the effective fps.</p>
<p>The  logfiles placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder store all the processing steps and related outputs for your reference.</p>
</div>
</div>
</div>
<div class="section" id="summary">
<h1>5 Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>The following <a class="reference external" href="files/doc/summary_results.xlsx">Excel table</a> summarizes the CNN features for each dataset and for each network in terms of:</p>
<ul class="simple">
<li><p>elapsed CPU time for the training process</p></li>
<li><p>number of CNN parameters and number of epochs for the training processed</p></li>
<li><p>TensorFlow output node names</p></li>
<li><p>top-1 accuracies estimated for the TF frozen graph and the quantized graph</p></li>
<li><p>top-1 accuracies measured on ZCU102 at run time execution</p></li>
<li><p>frames per second (fps) -measured on ZCU102 at run time execution- including reading the images with OpenCV function from ARM CPU (while in the real life case these images will be stored into DDR memory and so their access time should be negligible as seen from the DPU IP core).</p></li>
</ul>
<p><img alt="figure" src="../../../_images/summary_results.png" /></p>
<p>Note that in the case of CIFAR-10 dataset, being more sophisticated than the Fashion-MNIST, the top-1 accuracies of the four CNNs are quite different with <code class="docutils literal notranslate"><span class="pre">miniResNet</span></code> being the most accurate.</p>
<p>To save storage space, the folder <a class="reference external" href="files/target_zcu102">target_zcu102</a> contains only the <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> files for the CIFAR10 dataset, being more challenging and interesting than the Fashion-MNIST dataset.</p>
<p>The logfile related to the run on VCK190 board is placed in the <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder.</p>
</div>
<div class="section" id="references">
<h1>6 References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>https://www.pyimagesearch.com/2019/02/11/fashion-mnist-with-keras-and-deep-learning/</p></li>
<li><p>https://www.pyimagesearch.com/deep-learning-computer-vision-python-book/</p></li>
<li><p>https://github.com/Xilinx/Edge-AI-Platform-Tutorials/tree/master/docs/MNIST_tf</p></li>
<li><p>https://www.dlology.com/blog/how-to-convert-trained-keras-model-to-tensorflow-and-make-prediction/</p></li>
<li><p>https://github.com/Tony607/keras-tf-pb</p></li>
<li><p>https://towardsdatascience.com/image-classifier-cats-vs-dogs-with-convolutional-neural-networks-cnns-and-google-colabs-4e9af21ae7a8</p></li>
<li><p>https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</p></li>
<li><p>https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/</p></li>
<li><p>https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92</p></li>
<li><p>https://stats.stackexchange.com/questions/263349/how-to-convert-fully-connected-layer-into-convolutional-layer</p></li>
<li><p>https://www.tensorflow.org/guide/extend/model_files</p></li>
</ul>
</div>
<div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<div class="section" id="a1-build-and-run-on-vck190-target-board">
<h2>A1 Build and Run on VCK190 Target Board<a class="headerlink" href="#a1-build-and-run-on-vck190-target-board" title="Permalink to this headline">¶</a></h2>
<p>Alternatively to ZCU102, you can also use the <a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/vck190.html">VCK190</a> with its <a class="reference external" href="#">image file</a>, which contains a pre-built working design for the VCK190 with the DPUCVDX8G.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">xmodel</span></code> files generated for VCK190 are necessarily different from the ones of ZCU102, because the DPU architecture of the first board is different from the DPU of the second board. No changes to the C++ or Python files are needed for these four CNN examples.</p>
<p>Working with VCK190 board requires just to adopt the <code class="docutils literal notranslate"><span class="pre">6_compile_vai_vck190()</span></code> routine from the script <code class="docutils literal notranslate"><span class="pre">run_*.sh</span></code>, instead of the <code class="docutils literal notranslate"><span class="pre">6_compile_vai_zcu102()</span></code> which is related to ZCU102.</p>
<p>Make a <code class="docutils literal notranslate"><span class="pre">tar</span></code> file of the <code class="docutils literal notranslate"><span class="pre">target_vck190</span></code>  folder, copy it from the host PC to the target ZCU104 board. For example, in case of an Ubuntu PC, use the following command (assuming the board IP address is always the same):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="n">target_vck190</span><span class="o">.</span><span class="n">tar</span> <span class="n">root</span><span class="o">@</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.100</span><span class="p">:</span><span class="o">/</span><span class="n">root</span><span class="o">/</span>
<span class="n">cd</span> <span class="n">target_vck190</span>
<span class="n">source</span> <span class="o">./</span><span class="n">run_all_target</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>The  log files placed in <a class="reference external" href="files/rpt/ref_log">ref_log</a> folder contains all accuracy and fps performance for the CNNs.</p>
</div>
<div class="section" id="a2-pyimagesearch-permission">
<h2>A2 PyImageSearch Permission<a class="headerlink" href="#a2-pyimagesearch-permission" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>From: Adrian at PyImageSearch [mailto:a.rosebrock@pyimagesearch.com]
Sent: Thursday, February 20, 2020 12:47 PM
To: Daniele Bagni &lt;danieleb@xilinx.com&gt;
Subject: Re: URGENT: how to cite / use your code in my new DL tutorials

EXTERNAL EMAIL
Hi Daniele,

Yes, the MIT license is perfectly okay to use. Thank you for asking :-)

All the best,


From: Adrian at PyImageSearch &lt;a.rosebrock@pyimagesearch.com&gt;
Sent: Friday, April 12, 2019 4:25 PM
To: Daniele Bagni
Cc: danny.baths@gmail.com

Subject: Re: how to cite / use your code in my new DL tutorials

Hi Daniele,
Thanks for reaching out, I appreciate it! And yes, please feel free to use the code in your project.
If you could attribute the code to the book that would be perfect :-)
Thank you!
--
Adrian Rosebrock
Chief PyImageSearcher

On Sat, Apr 6, 2019 at 6:23 AM EDT, Daniele Bagni &lt;danieleb@xilinx.com&gt; wrote:

Hi Adrian.

...

Can I use part of your code in my tutorials?
In case of positive answer, what header do you want to see in the python files?

...


With kind regards,
Daniele Bagni
DSP / ML Specialist for EMEA
Xilinx Milan office (Italy)
</pre></div>
</div>
<hr/>
<p class="sphinxhide" align="center"><sup>Copyright&copy; 2020-2021 Xilinx</sup></p></div>
</div>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
															</div>
														</div>
														<div class="col-md-pull-6 col-lg-pull-6 col-md-6 col-lg-6">
															<span class="copyright">
                                  
                                  &copy; 2020–2021, Xilinx, Inc.
                              </span>
															<ul class="list-inline sub-menu">
																<li>
																	<a href="https://www.xilinx.com/about/privacy-policy.html">Privacy</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/legal.html">Legal</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/contact.html">Contact</a>
																</li>
															</ul>
														</div>
													</div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="quicklinks parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<noindex>
						<span class="quickLinks">
							<ul>
								<li>
									<a href="#top" class="btn backToTop">
									<span class="fas fa-angle-up" aria-hidden="true"></span>
									</a>
								</li>
							</ul>
						</span>
					</noindex>
				</div>
			</div>
		</div>
		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>