


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		
		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

  
  
  
  

  
      <script type="text/javascript" src="../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/header-footer.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../_static/js/Searchbox.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pro.min.css" media="all" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="header parbase aem-GridColumn aem-GridColumn--default--12">
								<noindex>
									<header data-component="header">
										<nav class="navbar navbar-default aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid main-nav">
													<div class="row">
														<div class="col-xs-12">
															<div class="logo-column">
																<div class="logo">
																	<a href="https://www.xilinx.com/">
																	<img src="https://www.xilinx.com/etc.clientlibs/site/clientlibs/xilinx/all/resources/imgs/header/xilinx-header-logo.svg" title="Xilinx Inc"/>
																	</a>
																</div>
															</div>
															<div class="navbar-column">
																<div class="navbar navbar-collapse collapse" id="xilinx-main-menu">
																	<div class="mobile-search-container">
																		<div id="headerSearchBox" class="headerSearch"
																			data-component="header-search"
																			data-redirect-if-empty="false"
																			data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																			data-coveo-organization-id="xilinxcomprode2rjoqok">
																			<div class='coveo-search-section'>
																				<div class="CoveoAnalytics" data-search-hub="Site"></div>
																				<ul class="dropdown-menu options">
																					<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																						<a href="#">
																						All</a>
																					</li>
																					<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com//products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Silicon Devices</a>
																					</li>
																					<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com//products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Boards and Kits</a>
																					</li>
																					<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com//products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																						<a href="#">
																						Intellectual Property</a>
																					</li>
																					<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																						<a href="#">
																						Support</a>
																						<ul>
																							<li data-label="Documentation" data-action-link="https://www.xilinx.com//support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																								<a href="#">
																								Documentation</a>
																							</li>
																							<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com//support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																								<a href="#">
																								Knowledge Base</a>
																							</li>
																							<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																								<a href="#">
																								Community Forums</a>
																							</li>
																						</ul>
																					</li>
																					<li data-label="Partners" data-action-link="https://www.xilinx.com//alliance/member-keyword-search.html" data-search-hub="Partner">
																						<a href="#">
																						Partners</a>
																					</li>
																					<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																						<a href="#">
																						Videos</a>
																					</li>
																					<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																						<a href="#">
																						Press</a>
																					</li>
																				</ul>
																				<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																				<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																			</div>
																		</div>
																	</div>
																	<ul class="nav navbar-nav nav-justified">
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/applications.html">
																			Applications</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/products/silicon-devices.html">
																			Products</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://developer.xilinx.com/">
																			Developers</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/support.html">
																			Support</a>
																		</li>
																		<li class="accordion-toggle-icons" data-component="toggle-dropdown">
																			<a href="https://www.xilinx.com/about/company-overview.html">
																			About</a>
																		</li>
																	</ul>
																</div>
															</div>
															<script type="text/javascript" src="../../../_static/js/gtm.js"></script>
															<!--<div class="mini-nav">
																<button type="button" data-function="xilinx-mobile-menu" id="nav-toggle" class="navbar-toggle collapsed visible-xs-block" aria-expanded="false">
																<span></span>
																<span></span>
																<span></span>
																<span></span>
																</button>
																<ul class="list-inline">
																	<li class="dropdown user-menu">
																		<button data-toggle="dropdown">
																		<span class="sr-only">Account</span>
																		<span class="fas fa-user"></span>
																		</button>
																		<ul class="dropdown-menu">
																			<li>
																				<a href="https://www.xilinx.com/myprofile/subscriptions.html">
																				My Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/registration/create-account.html">
																				Create Account</a>
																			</li>
																			<li>
																				<a href="https://www.xilinx.com/bin/protected/en/signout">
																				Sign Out</a>
																			</li>
																		</ul>
																	</li>
																	<li class="hidden-xs">
																		<button data-function="search-toggle">
																		<span class="sr-only">Search</span>
																		<span class="far fa-search"></span>
																		</button>
																	</li>
																</ul>
															</div>
															-->
															<div class="search-container">
																<div id="headerSearchBox" class="headerSearch"
																	data-component="header-search"
																	data-redirect-if-empty="false"
																	data-coveo-access-token="xxa237d4dd-f0aa-47fc-9baa-af9121851b33"
																	data-coveo-organization-id="xilinxcomprode2rjoqok">
																	<div class='coveo-search-section'>
																		<div class="CoveoAnalytics" data-search-hub="Site"></div>
																		<ul class="dropdown-menu options">
																			<li class="option" data-label="All" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-search-hub="Site">
																				<a href="#">
																				All</a>
																			</li>
																			<li data-label="Silicon Devices" data-action-link="https://www.xilinx.com/products/silicon-devices/si-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Silicon Devices</a>
																			</li>
																			<li data-label="Boards and Kits" data-action-link="https://www.xilinx.com/products/boards-and-kits/bk-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Boards and Kits</a>
																			</li>
																			<li data-label="Intellectual Property" data-action-link="https://www.xilinx.com/products/intellectual-property/ip-keyword-search.html" data-search-hub="Product">
																				<a href="#">
																				Intellectual Property</a>
																			</li>
																			<li data-label="Support" class="option" data-action-link="https://www.xilinx.com/search/support-keyword-search.html" data-search-hub="Support">
																				<a href="#">
																				Support</a>
																				<ul>
																					<li data-label="Documentation" data-action-link="https://www.xilinx.com/support/documentation-navigation/documentation-keyword-search.html" data-search-hub="Document">
																						<a href="#">
																						Documentation</a>
																					</li>
																					<li data-label="Knowledge Base" data-action-link="https://www.xilinx.com/support/answer-navigation/answer-keyword-search.html" data-search-hub="AnswerRecord">
																						<a href="#">
																						Knowledge Base</a>
																					</li>
																					<li data-label="Community Forums" data-action-link="https://www.xilinx.com/search/forums-keyword-search.html" data-search-hub="Forums">
																						<a href="#">
																						Community Forums</a>
																					</li>
																				</ul>
																			</li>
																			<li data-label="Partners" data-action-link="https://www.xilinx.com/alliance/member-keyword-search.html" data-search-hub="Partner">
																				<a href="#">
																				Partners</a>
																			</li>
																			<li data-label="Videos" data-action-link="https://www.xilinx.com/video/video-keyword-search.html" data-search-hub="Video">
																				<a href="#">
																				Videos</a>
																			</li>
																			<li data-label="Press" data-action-link="https://www.xilinx.com/search/press-keyword-search.html" data-search-hub="Press">
																				<a href="#">
																				Press</a>
																			</li>
																		</ul>
																		<a href="#" class="btn dropdown-toggle value" data-toggle="dropdown"></a>
																		<div class="CoveoSearchbox" data-id="coveosearchbox" data-action-link="https://www.xilinx.com/search/site-keyword-search.html" data-placeholder="Search Xilinx"></div>
																	</div>
																</div>
																<button data-function="search-toggle">
																<span class="sr-only">Search</span>
																<span class="far fa-times"></span>
																</button>
															</div>
														</div>
													</div>
												</div>
											</div>
										</nav>
									</header>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis In-Depth Tutorials
          

          
          </a>

          
            
            
              <div class="version">
                2020.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption"><span class="caption-text">日本語版</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs-jp/README.html">Master</a></li>
</ul>
<p class="caption"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis/README.html">Vitis Flow 101 Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis_HLS/README.html">Vitis HLS Analysis and Optimization</a></li>
</ul>
<p class="caption"><span class="caption-text">Machine Learning</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../README.html">Introduction to Machine Learning with Vitis AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Acceleration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html">Introduction to Vitis Hardware Accelerators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Hardware_Accelerators/README.html#feature-tutorials">Feature Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">AI Engine Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/README.html">AI Engine Development</a></li>
</ul>
<p class="caption"><span class="caption-text">Platform Creation Tutorial</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Vitis_Platform_Creation/README.html">Platform Creation</a></li>
</ul>
<p class="caption"><span class="caption-text">XRT and Vitis System Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Runtime_and_System_Optimization/README.html">Xilinx Runtime (XRT) and Vitis System Optimization Tutorials</a></li>
</ul>
<p class="caption"><span class="caption-text">Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/Xilinx/Vitis-Tutorials/blob/Vitis-Tutorials-2019.2-Hotfix1/README.md">2019.2</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../_sources/Machine_Learning/Design_Tutorials/07-yolov4-tutorial/README.md.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis In-Depth Tutorials</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>YOLOv4 Tutorials</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/Machine_Learning/Design_Tutorials/07-yolov4-tutorial/README.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="yolov4-tutorials">
<h1>YOLOv4 Tutorials<a class="headerlink" href="#yolov4-tutorials" title="Permalink to this headline">¶</a></h1>
<div class="section" id="intro">
<h2>Intro<a class="headerlink" href="#intro" title="Permalink to this headline">¶</a></h2>
<p>The following tutorials cover how to train, evaluate, convert, quantize, compile, and deploy Yolov4 on the Xilinx ZCU102 and ZCU104 evaluation boards.</p>
<p>We modified the official Yolov4 model config in order to compatible with the Xilinx Zynq Ultrascale+ Deep Learning Processor (DPU). These modifications typically result in a 1-2% accuracy loss vs. the original model.</p>
<p>Vitis-AI does not natively support Darknet and the trained Darknet model and can be converted with two options:</p>
<ul class="simple">
<li><p>TensorFlow using 3rd party repositories.</p></li>
<li><p>Caffe conversion using the Vitis-AI Darknet to Caffe conversion tool.</p></li>
</ul>
<p>Each  of the conversion floes is covered as a sperate Tutorial:</p>
<ul class="simple">
<li><p>Yolov4 trained on COCO and using conversion to TensorFLow</p></li>
<li><p>Yolov4 trained on VOC and using conversion to Caffe</p></li>
</ul>
<p>This tutorial is organized into the following steps:</p>
<p>1.)  Installation and Darknet Setup</p>
<p>2.) Training on Coco and Converting to TensorFLow</p>
<p>2.1) Darknet Model Training for Coco</p>
<p>2.2) Darknet Model Conversion to TensorFLow</p>
<p>2.3) Model Quantization and Compilation</p>
<p>2.3) Model Deployment on ZC102</p>
<p>3.) Training on VOC and Converting to  Caffe</p>
<p>3.1) Darknet Model Training for VOC</p>
<p>3.2) Darknet Model Conversion to Caffe</p>
<p>3.3) Model Qauntization</p>
<p>3.4) Model Evaluation</p>
<p>3.5) Model Compilation</p>
<p>3.6) Model Deploymnet on ZCU104</p>
</div>
<div class="section" id="general-installation-and-darknet-setup">
<h2>1.) General Installation and Darknet Setup<a class="headerlink" href="#general-installation-and-darknet-setup" title="Permalink to this headline">¶</a></h2>
<p>A few quick assumptions about the environment:</p>
<ul>
<li><p>Ubuntu 16.04, 18.04 or other linux Vitis-AI supported distributions.</p></li>
<li><p>If you’re expecting to do model training/evaluation, ensure you have NVidia Drivers, CUDA 10.0, and CuDNN installed to /usr/local/cuda (it can be a soft link)</p></li>
<li><p>The automated scripts use conda, so make sure you have Anaconda installed if you wish to use these.</p></li>
<li><p>I have opencv 4.4 installed.</p></li>
<li><p>This Tutorial has been tested on Vitis-AI 1.2</p></li>
<li><p>The first step is to clone and follow the install steps for Vitis-AI on the host machine: https://github.com/Xilinx/Vitis-AI</p></li>
<li><p>Next extract the contents of this tutorial within the Vitis-AI folder</p></li>
<li><p>The AlexyAB branch of Darknet is needed to train the yolov4 model:
<code class="docutils literal notranslate"> <span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/AlexeyAB/darknet</span></code></p></li>
<li><p>Next cd to the Darknet folder, open the Makefile, and make the following edits (assuming you have a CUDA capable GPU):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPU</span><span class="o">=</span><span class="mi">1</span>
<span class="n">CUDNN</span><span class="o">=</span><span class="mi">1</span>
<span class="n">CUDNN_HALF</span><span class="o">=</span><span class="mi">0</span>
<span class="n">OPENCV</span><span class="o">=</span><span class="mi">1</span>
<span class="n">LIBSO</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
</li>
<li><p>Build Darknet by running ‘make’ (you can use ‘make -jx’ where x is the number of processors you have).</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">make</span></code></p>
</div>
<div class="section" id="yolov4-using-coco-data-set-and-converting-to-tensorflow-tutorial">
<h2>2.) Yolov4 using COCO Data Set and converting to TensorFlow Tutorial<a class="headerlink" href="#yolov4-using-coco-data-set-and-converting-to-tensorflow-tutorial" title="Permalink to this headline">¶</a></h2>
<p>This Tutorial Covers the Darknet to TensorFlow process.</p>
</div>
<div class="section" id="darknet-model-training-on-coco">
<h2>2.1) Darknet Model Training on COCO<a class="headerlink" href="#darknet-model-training-on-coco" title="Permalink to this headline">¶</a></h2>
<p>To convert to TensorFlow you will also need the following repository:</p>
<ul class="simple">
<li><p>david8862 Keras Model Set: https://github.com/david8862/keras-YOLOv3-model-set</p></li>
</ul>
<p><code class="docutils literal notranslate"> <span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/david8862/keras-YOLOv3-model-set</span></code></p>
<p>Create, activate, and set up a virtual environment by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">./</span><span class="n">setup_environment</span><span class="o">.</span><span class="n">sh</span>
 <span class="n">conda</span> <span class="n">activate</span> <span class="n">yolov4</span>
 <span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
<p>For this tutorial I use the COCO 2017 dataset, though COCO 2014 will also work as well, and of course you can adapt it to whichever detection dataset makes sense for your application.</p>
<p>In terms of differences versus the official Yolov4 model, I made two main modifications to enable compliance with the Zynq Ultrascale+ DPU:</p>
<ul class="simple">
<li><p>The MISH activation layers are swapped to leakyrelu as the DPU doesn’t support MISH</p></li>
<li><p>The SPP module maxpool sizes have been changed to 5x5, 6x6, and 8x8 as the DPU has a maximum kernel size of 8x8</p></li>
</ul>
<p>After making these changes to the model and retraining on COCO 2014/2017, the yolov4 model achieves 63.4% mAP &#64; IOU50 and 41.3% AP using the COCO test2017 image set on the Codalab COCO evaluation server.  This is approximately a 2% degradation in mAP from the original model, though still a significant improvement over Yolov3 (~9% AP improvement).</p>
<p>I’ve provided a few scripts to help setup the COCO2017 dataset as Darknet requires a specific format for the labels.</p>
<p>You’ll need to download the train and validation images as well as the associated annotations:</p>
<ul class="simple">
<li><p>train images: http://images.cocodataset.org/zips/train2017.zip</p></li>
<li><p>val images: http://images.cocodataset.org/zips/val2017.zip</p></li>
<li><p>train/val annotations: http://images.cocodataset.org/annotations/annotations_trainval2017.zip</p></li>
</ul>
<p>Once you’ve extracted these files, you can use some of the provided scripts to process the annotations and generate the files needed for training within darknet.</p>
<ul class="simple">
<li><p>Under the scripts directory, I’ve provided two scripts “gen_yolo_train_labels.sh” and “gen_yolo_val_labels.sh”.  Open these two scripts and edit the paths to the annotations files as well as images.  The -labels argument provides a path where to write the txt files and darknet expects these to be in the same directory as the images.</p></li>
<li><p>After providing the specific paths to your dataset, run these two scripts - it will take some time (likely a couple hours) to process all the annotations.</p></li>
<li><p>Next you need to generate the list files val2017.txt and train2017.txt which are used by darknet to provide a list of images for training and validation.  I have provided a script (dk_files/gen_coco_lists.sh) that can be used for this purpose though you will need to open it and edit the paths to the validation and training datasets.</p></li>
<li><p>I have provided a pre-trained model under the dk_model directory.  This model has been trained using AlexayAB leakyrelu.weights as a starting point, then trained for about 200K iterations on COCO2014, and another ~20K Iterations on COCO2017.  The model weights are provided as a multi-part zip file, so you can just right click ‘yolov4-leaky_best.weights.7z.001’ and extract it to get the single combined weights file.</p></li>
<li><p>You can then run fine-tune training on the model in darknet by changing directory to “dk_files” and running <code class="docutils literal notranslate"><span class="pre">./train_yolov4.sh</span></code>.  Generally, when training, I set the model.cfg file input size to 512x512, and when deploying the model, it can be changed to a smaller input size such as 416x416 to increase FPS.</p>
<ul>
<li><p>I have also provided a script to measure map with Darknet directly and this is under ‘dk_files/dk_measure_map_yolov4.sh’.  This approach reports an mAP of about 70% &#64; 50IOU when using 512x512 input size.</p></li>
<li><p>When measuring accuracy with the pycocotools, I have observed that the mAP is about 60.2%. A script has been provided under scripts/dk_eval_yolov4_pycocotools_2017.py.  It’s possible that the difference lies in the evaluation code used by darknet vs. pycocotools which compares the output to the ground truths, though I haven’t investigated this extensively.</p>
<ul>
<li><p>This script uses the instances_val2017.json file to loop through each annotation, read the image, run inference in darknet and post processing, then write the results to a json file.  The results are then read back in at the end and evaluated with the pycocotools API.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="darknet-model-conversion">
<h2>2.2.) Darknet Model Conversion<a class="headerlink" href="#darknet-model-conversion" title="Permalink to this headline">¶</a></h2>
<p>The next step is to convert the darknet model to a frozen tensorflow graph.  The keras-YOLOv3-model-set repository provides some helpful scripts for this.</p>
<ul class="simple">
<li><p>In order to convert the model that is populated under dk_model, you can simply cd to the scripts directory and run ‘convert_yolov4.sh’.  This will create the keras.h5 model as well as the converted frozen TF graph under the tf_model folder.</p></li>
<li><p>In order to evaluate the converted model, I have provided a script ‘tf_eval_yolov4_coco_2017.py’ which can be used to evaluate the tensorflow frozen graph against the COCO2017 validation set.</p>
<ul>
<li><p>One key step is that this file expects the val2017 folder (containing the images for validation) and instances_val2017.json to be present under the scripts folder.  The reason for this is so that this script can be run with the quantized model within the docker container.  Make sure to copy these two items directly into the scripts folder before running the evaluation.</p></li>
<li><p>The mAP when measured on the frozen tf model with this approach is ~60.2% &#64; IOU50.</p></li>
<li><p>At this point the frozen tf graph under tf_model/tf_model.pb is ready for quantization/compilation.</p></li>
</ul>
</li>
</ul>
<div class="section" id="notes-on-alternative-conversion-flow">
<h3>Notes on Alternative Conversion Flow:<a class="headerlink" href="#notes-on-alternative-conversion-flow" title="Permalink to this headline">¶</a></h3>
<p>If you run into trouble converting your model with the david8862 Keras Model Set repository, there is an alternative approach for converting the model.  The alternative repository can be found here: https://github.com/qqwweee/keras-yolo3.</p>
<p>This repository has a similar script (convert.py) which can be used to convert from Darknet to Keras.  You will need to update the model_data/yolo_anchors.txt with the yolov4 anchors, but otherwise, I have tested this repository and it results in the exact same accuracy as the other conversion repository.</p>
<p>If you use this alternative approach for converting the model, you will still need to use convert from Keras to tensorflow which can be done using the other david8862 Keras Model Set repository Keras to tensorflow script.  You can reference the scripts/convert_yolov4.sh for an example command to do this step.</p>
<p>This repository results in different layer names, so in order to run the quantization, you’ll need to modify the quantize_yolov4.sh script and change the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Change –input_nodes to input_1
Change –output_nodes to conv2_94/BiasAdd,conv2_102/BiasAdd,conv2_110/BiasAdd
</pre></div>
</div>
<p>You’ll also need to modify the yolov4_graph_input_keras_fn.py to and change line 95 to:</p>
<p>return {“input_1”: images}</p>
<p>If you want to use the evaluation script “tf_eval_yolov4_coco_2017.py”, you will also need to make the same updates as noted above on lines 514-517.</p>
<p>When deploying the model, you’ll also need to update the dpu_yolov4.prototxt file which is under the dpu_yolov4 folder and change the layer_name parameters (increment them by 1).</p>
</div>
</div>
<div class="section" id="model-quantization-compilation">
<h2>2.3) Model Quantization/Compilation<a class="headerlink" href="#model-quantization-compilation" title="Permalink to this headline">¶</a></h2>
<p>At this point, you will need to start the docker container and activate the conda environment for tensorflow ‘conda activate vitis-ai-tensorflow’.</p>
<ul class="simple">
<li><p>Next install pycairo and pycocotools (both can be installed with pip).</p></li>
<li><p>If you followed the steps in the previous section, you should have the validation images in the scripts directory, if you didn’t go ahead and copy val2017 into the scripts directory</p></li>
<li><p>Change directory to the scripts folder and run <code class="docutils literal notranslate"><span class="pre">./quantize_yolov4.sh</span></code>.  This will produce a quantized graph in the yolov4_quantized directory a level above the scripts directory.</p></li>
<li><p>The quantized model can now be evaluated against the COCO2017 dataset by modifying ‘tf_eval_yolov4_coco_2017.py’ and setting quantized = ‘1’.</p></li>
<li><p>My quantized model achieves ~58.1% mAP &#64; IOU50 on COCO2017 validation set.</p></li>
<li><p>The model can now be compiled for the ZCU102 by running <code class="docutils literal notranslate"><span class="pre">./compile_yolov4.sh</span></code> from the scripts directory</p></li>
</ul>
</div>
<div class="section" id="model-deployment">
<h2>2.4) Model Deployment<a class="headerlink" href="#model-deployment" title="Permalink to this headline">¶</a></h2>
<p>Once you’ve set up your board image to run the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/master/Vitis-AI-Library">Vitis-AI-Library</a> examples, you can proceed to the following steps.</p>
<p>The code used to run yolov3 from the Vitis-AI-Library can also be used to run yolov4.  If the target has been setup correctly, this should be present under ~/Vitis-AI/vitis_ai_library/samples/yolov3</p>
<ul class="simple">
<li><p>On the host machine copy the compiled model .elf file (or use my pre-trained one already present there) to the the dpu_yolov4 directory.</p>
<ul>
<li><p>If using my pretrained model, you’ll need to extract it by right clicking “dpu_yolov4.elf.7z.001” and selecting ‘extract’.</p></li>
<li><p>Otherwise, just copy your elf file to the dpu_yolov4 directory</p></li>
</ul>
</li>
<li><p>Next copy the entire dpu_yolov4 directory into the yolov3 samples directory on the target ZCU102 board.</p></li>
<li><p>Build the yolov3 example by running ‘bash -x build.sh’</p></li>
<li><p>Run with 6 threads with a webcam using:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">./</span><span class="n">test_video_yolov3</span> <span class="n">dpu_yolov4</span> <span class="mi">0</span> <span class="o">-</span><span class="n">t</span> <span class="mi">6</span>
</pre></div>
</div>
<p>In this short tutorial, we’ve covered how to train, evaluate, convert, quantize and deploy a dpu compatible version of yolov4 the ZCU102 board.  Good luck and happy developing!</p>
</div>
<div class="section" id="yolov4-using-voc-data-set-and-converting-to-caffe-tutorial">
<h2>3.0) Yolov4 using VOC Data Set and converting to Caffe Tutorial<a class="headerlink" href="#yolov4-using-voc-data-set-and-converting-to-caffe-tutorial" title="Permalink to this headline">¶</a></h2>
<p>This tutorial covers the Darknet to Caffe process.</p>
</div>
<div class="section" id="darknet-model-training-on-voc">
<h2>3.1) Darknet Model Training on VOC<a class="headerlink" href="#darknet-model-training-on-voc" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>This Tutorial uses the VOC dataset. You can run the following script to download the dataset and ground truth files. A python file is used to create the labels so you will need to create a python env if you do not have one:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">create</span> <span class="o">-</span><span class="n">n</span> <span class="n">yolov4</span> <span class="n">pip</span> <span class="n">python</span><span class="o">=</span><span class="mf">3.6</span>
<span class="n">conda</span> <span class="n">activate</span>  <span class="n">yolov4</span>
<span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">get_voc_dataset</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Next you will need to modify the configuration file <strong>darknet/cfg/yolov4.cfg</strong> to be compatible with the Zynq Ultrascale+ DPU. The MISH activation layers are swapped to leaky as the DPU doesn’t support MISH. The SPP module maxpool section has kernel sizes of 5, 9, 13, and the DPU only supports a maxpool kernel size of upto 8. There are also additional restrictions for maxpool when the stride=1 and converting to Caffe. There are 3 possible changes you can make:</p>
<ul>
<li><p>Change all max kernel sizes to 1, 1, 1</p></li>
<li><p>Change all kernel sizes 3,5, 7</p></li>
<li><p>Comment out max_pool layers.</p></li>
</ul>
</li>
<li><p>I saw the best post Quantization accuracy results when commenting out the 3 max_poolong layers as follows:</p></li>
</ul>
<p><img alt="../../../_images/cfg_mod.png" src="../../../_images/cfg_mod.png" /></p>
<ul class="simple">
<li><p>Make the following additional changes:</p>
<ul>
<li><p>set height to 416 and width to 416</p></li>
<li><p>set max_batches to 1001000</p></li>
<li><p>set number of classes to 20 for each output layer</p></li>
<li><p>set filter size of convolutional layer before each output layers to 75</p></li>
</ul>
</li>
<li><p>Note that the batch size divided by the subdivision is the number of images sent to the GPU at one time. I used a subdivsion value of 16. If your GPU has enough memory you can try reducing the subdivision size to a smaller number for faster training. If you get a GPU memory error during training you will need to go back and increase the subdivision value.</p></li>
<li><p>Make the following changes to the <strong>darknet/cfg/voc.data</strong> file:</p></li>
</ul>
<p><img alt="../../../_images/voc_data.png" src="../../../_images/voc_data.png" /></p>
<ul class="simple">
<li><p>Download Yolov4 pretrained weights. From within the darknet directory run:</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">wget</span> <span class="pre">https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights</span></code></p>
<ul class="simple">
<li><p>Launch Training:</p></li>
</ul>
<p><code class="docutils literal notranslate"> <span class="pre">./darknet</span> <span class="pre">detector</span> <span class="pre">train</span> <span class="pre">cfg/voc.data</span> <span class="pre">cfg/yolov4.cfg</span> <span class="pre">yolov4.weights</span> <span class="pre">-map</span></code></p>
<ul class="simple">
<li><p>It should take about 150K Iterations for the training to converge. To measure accuracy you can use the following command:</p></li>
</ul>
<p><code class="docutils literal notranslate"> <span class="pre">./darknet</span> <span class="pre">detector</span> <span class="pre">map</span>&#160; <span class="pre">cfg/voc.data</span> <span class="pre">cfg/yolov4.cfg</span> <span class="pre">backup/yolov4_best.weights</span> <span class="pre">-letterbox</span> <span class="pre">-point</span> <span class="pre">11</span></code></p>
<ul class="simple">
<li><p>For my trained model I saw:</p>
<ul>
<li><p>mAP for model with modifications              82.39%</p></li>
<li><p>mAP for orginal model with no modification    82.55%</p></li>
</ul>
</li>
<li><p>The model weights are provided as a multi-part zip file, so you can just right click ‘yolov4_voc_best.weights.7z.001’ and extract it to get the single combined weights file.</p></li>
</ul>
</div>
<div class="section" id="id1">
<h2>3.2) Darknet Model Conversion<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The next step is to convert the darknet model to a Caffe model. This will be done using the built in conversion function which is part of the Vitis-AI Caffe install.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="o">&lt;</span><span class="n">Path</span> <span class="n">to</span> <span class="n">Vitis</span><span class="o">-</span><span class="n">AI</span> <span class="n">Install</span><span class="o">&gt;/</span><span class="n">Vitis</span><span class="o">-</span><span class="n">AI_1</span><span class="o">.</span><span class="mi">2</span><span class="o">/</span><span class="n">docker_run</span><span class="o">.</span><span class="n">sh</span> <span class="n">xilinx</span><span class="o">/</span><span class="n">vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">gpu</span><span class="p">:</span><span class="n">latest</span>
<span class="n">conda</span> <span class="n">activate</span> <span class="n">vitis</span><span class="o">-</span><span class="n">ai</span><span class="o">-</span><span class="n">caffe</span>
<span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">darknet_convert</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div>
<div class="section" id="model-quantization">
<h2>3.3) Model Quantization<a class="headerlink" href="#model-quantization" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Before quantizing  the model, we will need to make a minor modifcations to .prototxt file to point to the calibration images. Make a new copy of the prototxt file and make  the following edits:</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">voc/yolov4.prototxt</span> <span class="pre">voc/yolov4_quant.prototxt</span></code></p>
<p><img alt="../../../_images/quant_mods.png" src="../../../_images/quant_mods.png" /></p>
<p>FOr your convience here are the lines to copy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;Darkent2Caffe&quot;</span>
  <span class="c1">#input: &quot;data&quot;</span>
  <span class="c1">#input_dim: 1</span>
  <span class="c1">#input_dim: 3</span>
  <span class="c1">#input_dim: 416</span>
  <span class="c1">#input_dim: 416</span>

  <span class="c1">####Change input data layer to VOC validation images #####</span>
  <span class="n">layer</span> <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>
    <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;ImageData&quot;</span>
    <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>
    <span class="n">top</span><span class="p">:</span> <span class="s2">&quot;label&quot;</span>
    <span class="n">include</span> <span class="p">{</span>
      <span class="n">phase</span><span class="p">:</span> <span class="n">TRAIN</span>
    <span class="p">}</span>
    <span class="n">transform_param</span> <span class="p">{</span>
      <span class="n">mirror</span><span class="p">:</span> <span class="n">false</span>
      <span class="n">yolo_height</span><span class="p">:</span><span class="mi">416</span>  <span class="c1">#change height according to Darknet model</span>
      <span class="n">yolo_width</span><span class="p">:</span><span class="mi">416</span>   <span class="c1">#change width according to Darknet model</span>
    <span class="p">}</span>
    <span class="n">image_data_param</span> <span class="p">{</span>
      <span class="n">source</span><span class="p">:</span> <span class="s2">&quot;model_data/calib.txt&quot;</span>  <span class="c1">#list of calibration imaages</span>
      <span class="n">root_folder</span><span class="p">:</span> <span class="s2">&quot;images/&quot;</span> <span class="c1">#path to calibartion images</span>

      <span class="n">batch_size</span><span class="p">:</span> <span class="mi">1</span>
      <span class="n">shuffle</span><span class="p">:</span> <span class="n">false</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="c1">#####No changes to the below layers#####</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">bash</span> <span class="pre">scripts/run_via_q.sh</span></code></p>
</div>
<div class="section" id="compare-accuracy-between-floating-point-and-quantized-models-optional">
<h2>3.4) Compare Accuracy Between Floating Point and Quantized Models (Optional)<a class="headerlink" href="#compare-accuracy-between-floating-point-and-quantized-models-optional" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">test_caffe_fp</span><span class="o">.</span><span class="n">sh</span>
<span class="n">bash</span> <span class="n">evaluate_caffe_fp</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The mAP value for our trained model was 84.03%</p></li>
</ul>
<p><img alt="../../../_images/fp_map.png" src="../../../_images/fp_map.png" /></p>
<div class="section" id="generate-predictions-using-quantized-model">
<h3><strong>Generate predictions using quantized model</strong><a class="headerlink" href="#generate-predictions-using-quantized-model" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>In order to run predictions on the quantized model, we first will need to make some minor edits to the simulation prototxt file quant_output/quantize_train_test.prototxt Make the following edits to input layer so  that you file looks like the following:</p></li>
</ul>
<p><img alt="../../../_images/sim_model_mods.png" src="../../../_images/sim_model_mods.png" /></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">test_quantized</span><span class="o">.</span><span class="n">sh</span>
<span class="n">bash</span> <span class="n">scripts</span><span class="o">/</span><span class="n">evaluate_quantized</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The map value for our quantized model was 82.48%</p></li>
</ul>
<p><img alt="../../../_images/int8_map.png" src="../../../_images/int8_map.png" /></p>
</div>
</div>
<div class="section" id="model-compliation">
<h2>3.5) Model Compliation<a class="headerlink" href="#model-compliation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prototxt-file-mods">
<h3><strong>Prototxt File Mods</strong><a class="headerlink" href="#prototxt-file-mods" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Before compiling the model we will need to make a minor edit to the deploy prototxt file <strong>yolov4_quantized/deploy.prototxt</strong>. Comment out lines 5-9 as shown below:</p></li>
</ul>
<p><img alt="../../../_images/deploy_mods.png" src="../../../_images/deploy_mods.png" /></p>
<p><code class="docutils literal notranslate"> <span class="pre">bash</span> <span class="pre">scripts/run_via_c_zu104.sh</span></code></p>
<p>The compiled elf file will be located in yolov4_compiled/dpu_yolov4_voc.elf</p>
</div>
</div>
<div class="section" id="running-the-vitis-ai-library-examples">
<h2>3.6) Running  the Vitis AI Library Examples<a class="headerlink" href="#running-the-vitis-ai-library-examples" title="Permalink to this headline">¶</a></h2>
<p>Once you’ve set up your board image to run the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/master/Vitis-AI-Library">Vitis-AI-Library</a> examples, you can proceed to the following steps.</p>
<p>The code used to run yolov3 from the Vitis-AI-Library can also be used to run yolov4.  If the target has been setup correctly, this should be present under ~/Vitis-AI/vitis_ai_library/samples/yolov3</p>
<ul class="simple">
<li><p>On the host machine copy the compiled model .elf file (or use my pre-trained one already present there) to the the dpu_yolov4_voc directory.</p>
<ul>
<li><p>If using my pretrained model, you’ll need to extract it by right clicking “dpu_yolov4_voc.elf.7z.001” and selecting ‘extract’.</p></li>
<li><p>Otherwise, just copy your elf file to the dpu_yolov4_voc directory</p></li>
</ul>
</li>
<li><p>Next copy the entire dpu_yolov4 directory into the yolov3 samples directory on the target ZCU104 board.</p></li>
<li><p>If you would like to run the yolov3 accuracy example you will need to make the following edit to line 85 of the file: <strong>yolov3_voc_accuracy.cpp</strong>
<img alt="../../../_images/acc_mod.png" src="../../../_images/acc_mod.png" /></p></li>
<li><p>Build the yolov3 example by running ‘bash -x build.sh’</p></li>
</ul>
<div class="section" id="running-the-yolo-video-test">
<h3>Running the Yolo Video Test<a class="headerlink" href="#running-the-yolo-video-test" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>To run the Yolo Video Test webcam using 4 threads use the following command. Assuming that you have a DP Monitor connected, you will also the DISPLAY variable first.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">DISPLAY</span><span class="o">=</span><span class="p">:</span><span class="mf">0.0</span>
<span class="o">./</span><span class="n">test_video_yolov3</span> <span class="n">dpu_yolov4_voc</span> <span class="mi">0</span> <span class="o">-</span><span class="n">t4</span>
</pre></div>
</div>
</div>
<div class="section" id="running-the-yolo-accuracy-test">
<h3>Running the Yolo Accuracy Test<a class="headerlink" href="#running-the-yolo-accuracy-test" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>To run the Yolo Accuarcy test against the VOC data test set, you will need to copy the images directory from the host machine to the target ZCU104 board. In addition copy the image list file voc/2007_test.txt to the targt board.</p></li>
<li><p>Change the conf_threshold value in file <strong>yolov4_voc.prototxt</strong> to 0.005</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">./test_accuracy_yolov3_voc</span> <span class="pre">2007_test.txt</span> <span class="pre">yolov4_voc_pred.txt</span></code></p>
<ul class="simple">
<li><p>This will take a few minutes to run. When the predictions complete, copy the file <strong>yolov4_pred.txt</strong> to your host machine and run the evaluation from within your Vitis-AI Caffe conda environment:</p></li>
</ul>
<p><code class="docutils literal notranslate"> <span class="pre">bash</span> <span class="pre">./scripts/evaluate_board.sh</span></code></p>
<ul class="simple">
<li><p>The mAP reported should be close to what was reported for the Quantized mAP</p></li>
</ul>
</div>
<div class="section" id="running-the-yolo-performance-test">
<h3>Running the Yolo Performance test<a class="headerlink" href="#running-the-yolo-performance-test" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>To run the Yolo Performance Test, create a file called image.list on the ZCU104, and add the first 5 images names from the file 2007_test.txt. To run with 4 threads use the following command:</p>
<ul>
<li><p>Running on a  ZCU104 you should see a performance of 29.7FPS.</p></li>
</ul>
</li>
</ul>
<p><code class="docutils literal notranslate"> <span class="pre">./test_performance_yolov3</span> <span class="pre">dpu_yolov4_voc</span> <span class="pre">image.list</span> <span class="pre">-t4</span> <span class="pre">-s30</span></code></p>
<p>In this short tutorial, we’ve covered how to train, evaluate, convert, quantize and deploy a dpu compatible version of yolov4 on the ZCU104 board.  Good luck and happy developing!</p>
<p align=center><sup>Copyright&copy; 2020 Xilinx Inc.</sup></p><p align=cener><sup>
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at: <a href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a>. Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.
</sup></p></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
															</div>
														</div>
														<div class="col-md-pull-6 col-lg-pull-6 col-md-6 col-lg-6">
															<span class="copyright">
                                  
                                  &copy; 2020–2021, Xilinx, Inc.
                              </span>
															<ul class="list-inline sub-menu">
																<li>
																	<a href="https://www.xilinx.com/about/privacy-policy.html">Privacy</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/legal.html">Legal</a>
																</li>
																<li>
																	<a href="https://www.xilinx.com/about/contact.html">Contact</a>
																</li>
															</ul>
														</div>
													</div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
				<div class="quicklinks parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<noindex>
						<span class="quickLinks">
							<ul>
								<li>
									<a href="#top" class="btn backToTop">
									<span class="fas fa-angle-up" aria-hidden="true"></span>
									</a>
								</li>
							</ul>
						</span>
					</noindex>
				</div>
			</div>
		</div>
		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>