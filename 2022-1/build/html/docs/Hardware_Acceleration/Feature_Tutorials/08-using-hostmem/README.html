<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
  <title>Host Memory Access &mdash; Vitis™ Tutorials 2022.1 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Using GT Kernel in Alveo with Vitis Flow" href="../09-using-ethernet-on-alveo/README.html" />
    <link rel="prev" title="Using HBM" href="../07-using-hbm/README.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../../../index.html" class="icon icon-home"> Vitis™ Tutorials
            <img src="../../../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2022.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">日本語版</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs-jp/index.html">Main</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Getting_Started/Vitis-Getting-Started.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Acceleration</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../Hardware-Acceleration.html">Hardware Acceleration</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../Hardware-Acceleration.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hardware-Acceleration.html#design-tutorials">Design Tutorials</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../Hardware-Acceleration.html#feature-tutorials">Feature Tutorials</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../01-rtl_kernel_workflow/README.html">Getting Started with RTL Kernels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02-mixing-c-rtl-kernels/README.html">Mixing C and RTL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../03-dataflow_debug_and_optimization/README.html">Dataflow Debug and Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../04-mult-ddr-banks/README.html">Using Multiple DDR Banks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../05-using-multiple-cu/README.html">Using Multiple Compute Units</a></li>
<li class="toctree-l3"><a class="reference internal" href="../06-controlling-vivado-implementation/README.html">Controlling Vivado Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../07-using-hbm/README.html">Optimizing for HBM</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Host Memory Access</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xrt-and-platform-version">XRT and Platform version</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial-description">Tutorial Description</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#kernel-structure">Kernel structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="#host-code">Host code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kernel-compilation">Kernel compilation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-application">Running the application</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../09-using-ethernet-on-alveo/README.html">Using GT Kernels and Ethernet IPs on Alveo</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AI Engine</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../AI_Engine_Development/AI_Engine_Development.html">AI Engine Development</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Platforms</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../Vitis_Platform_Creation/Vitis_Platform_Creation.html">Vitis Platform Creation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2021-2/build/html/index.html">2021.2</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2021-1/build/html/index.html">2021.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-2/docs/index.html">2020.2</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/2020-1/docs/README.html">2020.1</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Vitis™ Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../Hardware-Acceleration.html">Vitis Hardware Acceleration</a> &raquo;</li>
      <li>Host Memory Access</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/docs/Hardware_Acceleration/Feature_Tutorials/08-using-hostmem/README.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="host-memory-access">
<h1>Host Memory Access<a class="headerlink" href="#host-memory-access" title="Permalink to this heading">¶</a></h1>
<p><strong>Version:</strong> Vitis 2022.1</p>
<p>Some of the recent Xilinx Platforms have an XDMA feature to bypass the DMA operation and allow the kernels to directly access the host memory. The direct host memory access provides an alternate data transfer mechanism compared to XDMA based data transfer and can be useful in some of the scenarios.</p>
<ul class="simple">
<li><p>Custom kernel developers can use their already developed DMA engine as part of their kernel design. This specifically helps RTL kernel developers who are looking to reuse existing DMA IP.</p></li>
<li><p>A platform can be designed even without an XDMA, such as the latest U50-NoDMA platform, providing a thin fixed logic with more available FPGA resources for the user logic or kernels.</p></li>
<li><p>The data transfer burden can be freed from the host server. As a result, the host can spend time on other tasks that might even help in overall application performance.</p></li>
</ul>
<p>For more details about the host memory access feature please refer <a class="reference external" href="https://xilinx.github.io/XRT/master/html/hm.html">https://xilinx.github.io/XRT/master/html/hm.html</a></p>
</div>
<div class="section" id="xrt-and-platform-version">
<h1>XRT and Platform version<a class="headerlink" href="#xrt-and-platform-version" title="Permalink to this heading">¶</a></h1>
<p>The following XRT and U250 platform versions are used for this tutorial design.</p>
<p><strong>XRT Version</strong>:    2022.1</p>
<p><strong>Platform</strong>: xilinx_u250_gen3x16_xdma_3_1_202020_1</p>
</div>
<div class="section" id="tutorial-description">
<h1>Tutorial Description<a class="headerlink" href="#tutorial-description" title="Permalink to this heading">¶</a></h1>
<p>This tutorial attempts to demonstrate a scenario where adopting a host memory access data transfer scheme helped to improve overall performance. This is not a full-fledged application, rather a small example showing potential performance benefit by letting direct host memory access by the kernels.</p>
<p>A word of caution is that the performance numbers can greatly varied from one host server to another host server, so performance numbers demonstrated in this example should only be used as sample data that may not match when you are running this tutorial.</p>
<div class="section" id="kernel-structure">
<h2>Kernel structure<a class="headerlink" href="#kernel-structure" title="Permalink to this heading">¶</a></h2>
<p>This tutorial is created from a simple vector-add kernel. The vector-add kernel has 15 CUs on the FPGA. As the U250 card contains four SLRs, the 15 CUs are distributed in the following manner</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 36%" />
<col style="width: 64%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>SLR</p></th>
<th class="head"><p>Number of CUs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SLR0</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>SLR1</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>SLR2</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>SLR3</p></td>
<td><p>4</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="host-code">
<h2>Host code<a class="headerlink" href="#host-code" title="Permalink to this heading">¶</a></h2>
<p>The functionality of the host code is described as below:</p>
<ul class="simple">
<li><p>Creates dedicated 15 CU handles for 15 CUs</p></li>
<li><p>Submits 15 CU execution requests</p></li>
<li><p>When a CU is finished, it is executed again. In this way, all 15 CUs kept running.</p></li>
<li><p>The above process continues for a certain time, in this example for 20 seconds.</p></li>
<li><p>After 20 seconds host code calculates the total number of completed CU executions.</p></li>
</ul>
<p>The greater number of CU executions in a given time interval signifies the more work done with increased throughput.</p>
<p>As a side note, for brevity, the host code is simplified by using the same data input per CU execution. As the host code is solely focusing on CU execution, it is a simplified version without implementing other typical host functionalities such as verification of returned data from the CUs, error checking, etc.</p>
</div>
<div class="section" id="kernel-compilation">
<h2>Kernel compilation<a class="headerlink" href="#kernel-compilation" title="Permalink to this heading">¶</a></h2>
<p>All the tutorial design files are self-contained inside the <code class="docutils literal notranslate"><span class="pre">reference-files</span></code> directory.</p>
<p>This tutorial needs two kernel XCLBINs with different connectivities.</p>
<ul class="simple">
<li><p>XCLBIN 1: All the AXI master ports are connected to DDR banks</p></li>
<li><p>XCLBIN 2: All the AXI master ports are connected to the host memory</p></li>
</ul>
<p>Note and compare the kernel link configuration files for the above two cases.  The kernel link configuration file is the only difference between the two kernel compilation flows. The top portion of the .cfg files (<code class="docutils literal notranslate"><span class="pre">./src/link.cfg</span></code> and <code class="docutils literal notranslate"><span class="pre">./src/link_hm.cfg</span></code>) is the same where 15 CUs are named and placed in different SLRs. However, the bottom half of the .cfg files are different as shown below</p>
<p>The bottom half of the v++ link file <code class="docutils literal notranslate"><span class="pre">./src/link.cfg</span></code> for DDR connected CUs</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">connectivity</span><span class="p">]</span>
<span class="o">....</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_1</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_2</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_3</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_4</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_5</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_6</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_7</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_8</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_9</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_10</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_11</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_12</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_13</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_14</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_15</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">DDR</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>The bottom half of the v++ link file <code class="docutils literal notranslate"><span class="pre">./src/link_hm.cfg</span></code> for host memory connected CUs</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">connectivity</span><span class="p">]</span>
<span class="o">....</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_1</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_2</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_3</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_4</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_5</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_6</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_7</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_8</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_9</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_10</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_11</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_12</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_13</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_14</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sp</span><span class="o">=</span><span class="n">vadd_15</span><span class="o">.</span><span class="n">m_axi_gmem</span><span class="p">:</span><span class="n">HOST</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>The Makefile is using <code class="docutils literal notranslate"><span class="pre">./src/link.cfg</span></code> file by default. To build the DDR connected kernel XCLBIN simply do</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">xclbin</span>
</pre></div>
</div>
<p>Upon completion, you will get the XCLBIN file <code class="docutils literal notranslate"><span class="pre">vadd.hw.run1.xclbin</span></code>. The Makefile specifies LAB=run1 as the default flow.</p>
<p>Next, to change v++ configuration file simply run LAB=run2 as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">xclbin</span> <span class="n">LAB</span><span class="o">=</span><span class="n">run2</span>
</pre></div>
</div>
<p>Upon completion, you will get the XCLBIN file <code class="docutils literal notranslate"><span class="pre">vadd.hw.run2.xclbin</span></code>.</p>
<p>Once you have two XCLBINs ready you can simply focus on running the application for the rest of this tutorial.</p>
</div>
</div>
<div class="section" id="running-the-application">
<h1>Running the application<a class="headerlink" href="#running-the-application" title="Permalink to this heading">¶</a></h1>
<p><strong>DDR Based Run</strong></p>
<p>You will start with the DDR-based application to see the result.</p>
<p>Compile and run the host code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">exe</span>
<span class="o">./</span><span class="n">host</span><span class="o">.</span><span class="n">exe</span> <span class="n">vadd</span><span class="o">.</span><span class="n">hw</span><span class="o">.</span><span class="n">run1</span><span class="o">.</span><span class="n">xclbin</span>
</pre></div>
</div>
<p>The run will take around 20+ seconds as this application is running for 20 seconds and counting the total number of CU executions during this time interval.  You will see an output similar below</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Buffer</span> <span class="n">Inputs</span> <span class="mi">2</span> <span class="n">MB</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="mi">2702</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="mi">2700</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span><span class="mi">2700</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span><span class="mi">2702</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span><span class="mi">2701</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span><span class="mi">2698</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span><span class="mi">2698</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">11</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">12</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">13</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">14</span><span class="p">]:</span><span class="mi">2699</span>
<span class="n">Total</span> <span class="n">Kernel</span> <span class="n">execution</span> <span class="ow">in</span> <span class="mi">20</span> <span class="n">seconds</span><span class="p">:</span><span class="mi">40493</span>

<span class="n">Data</span> <span class="n">processed</span> <span class="ow">in</span> <span class="mi">20</span> <span class="n">seconds</span><span class="p">:</span> <span class="mi">4</span><span class="n">MB</span><span class="o">*</span><span class="n">total_kernel_executions</span><span class="p">:</span><span class="mi">161972</span> <span class="n">MB</span>

<span class="n">Data</span> <span class="n">processed</span><span class="o">/</span><span class="n">sec</span> <span class="p">(</span><span class="n">GBPs</span><span class="p">)</span><span class="o">=</span> <span class="mf">8.0986</span> <span class="n">GBPs</span>
<span class="n">TEST</span> <span class="n">SUCCESS</span>
</pre></div>
</div>
<p>Please note that the number of exact kernel executions can be varied depending on the host server capability and you may see different numbers from the above. In the sample run above it shows that each CUs are executed almost same number of times (~2700) during the 20 second time interval. The total number of CU executions is around 40K.</p>
<p>The host code also calculates the application throughput that depends on the number of total CU executions. As each CU processed 4MB of data the throughput of the application as calculated above is approximately 8GBPs</p>
<p>You will invoke the <code class="docutils literal notranslate"><span class="pre">vitis_analyzer</span></code> by using the .run_summary file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vitis_analyzer</span> <span class="n">vadd</span><span class="o">.</span><span class="n">hw</span><span class="o">.</span><span class="n">run1</span><span class="o">.</span><span class="n">xclbin</span><span class="o">.</span><span class="n">run_summary</span>
</pre></div>
</div>
<p>In the Profile Report tab, select <strong>Profile Summary</strong> from the left panel followed by <strong>Kernel and Compute Units</strong> section. You can see all the CU and their execution numbers that you have already seen from the stdout from the host application run. The following snapshot also shows every CU’s average execution time close to 1ms.</p>
<img alt="../../../../_images/ddr_profile.JPG" class="align-center" src="../../../../_images/ddr_profile.JPG" />
<p>You can also review the <strong>Host Transfer</strong> section that shows the transfer rate between Host and Global Memory. The host code is transferring 4 MB of data before every CU execution and transferring back 2 MB of data after every CU execution.</p>
<img alt="../../../../_images/ddr_host_transfer.JPG" class="align-center" src="../../../../_images/ddr_host_transfer.JPG" />
<p>Now select the <strong>Application Timeline</strong> section from the left panel. The application timeline also shows the large data transfers initiated by the host server that supposed to keep the host server busy. As shown below hovering the mouse on one of the data transfers showing a typical DMA writes for 4MB data from the host is taking approximately 1ms.</p>
<img alt="../../../../_images/at_ddr.JPG" class="align-center" src="../../../../_images/at_ddr.JPG" />
<p>This is also interesting to note the number of parallel requests by the host to submit the CU execution commands. For example, the above Application timeline snapshot shows 4 such parallel execution command requests (under <strong>Kernel Enqueues</strong> Row 0, Row 1, Row 2, and Row 3).</p>
<p><strong>Host Memory Based Run</strong></p>
<p>The host code used for the host memory-based run is <code class="docutils literal notranslate"><span class="pre">host_hm.cpp</span></code>. The only host code change is specifying the buffers as host memory buffers as below. The host code sets <code class="docutils literal notranslate"><span class="pre">cl_mem_ext_ptr_t.flag</span></code> to <code class="docutils literal notranslate"><span class="pre">XCL_MEM_EXT_HOST_ONLY</span></code> to denote a host memory buffer.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">cl_mem_ext_ptr_t</span><span class="w"> </span><span class="n">host_buffer_ext</span><span class="p">;</span><span class="w"></span>
<span class="n">host_buffer_ext</span><span class="p">.</span><span class="n">flags</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">XCL_MEM_EXT_HOST_ONLY</span><span class="p">;</span><span class="w"></span>
<span class="n">host_buffer_ext</span><span class="p">.</span><span class="n">obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">NULL</span><span class="p">;</span><span class="w"></span>
<span class="n">host_buffer_ext</span><span class="p">.</span><span class="n">param</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="n">in1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clCreateBuffer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="n">CL_MEM_READ_ONLY</span><span class="o">|</span><span class="n">CL_MEM_EXT_PTR_XILINX</span><span class="p">,</span><span class="n">bytes</span><span class="p">,</span><span class="o">&amp;</span><span class="n">host_buffer_ext</span><span class="w"></span>
<span class="n">throw_if_error</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="s">&quot;failed to allocate in buffer&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">in2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clCreateBuffer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="n">CL_MEM_READ_ONLY</span><span class="o">|</span><span class="n">CL_MEM_EXT_PTR_XILINX</span><span class="p">,</span><span class="n">bytes</span><span class="p">,</span><span class="o">&amp;</span><span class="n">host_buffer_ext</span><span class="w"></span>
<span class="n">throw_if_error</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="s">&quot;failed to allocate in buffer&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">io</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">clCreateBuffer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="n">CL_MEM_WRITE_ONLY</span><span class="o">|</span><span class="n">CL_MEM_EXT_PTR_XILINX</span><span class="p">,</span><span class="n">bytes</span><span class="p">,</span><span class="o">&amp;</span><span class="n">host_buffer_ext</span><span class="w"></span>
<span class="n">throw_if_error</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="s">&quot;failed to allocate io buffer&quot;</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Before running the host memory-based application ensure that you have preconfigured and preallocated the host memory for CU access. For this testcase setting a host memory size of 1G is sufficient.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">xilinx</span><span class="o">/</span><span class="n">xrt</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">xbutil</span> <span class="n">host_mem</span> <span class="o">--</span><span class="n">enable</span> <span class="o">--</span><span class="n">size</span> <span class="mi">1</span><span class="n">G</span>
</pre></div>
</div>
<p>Compile and run the host code</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">exe</span> <span class="n">LAB</span><span class="o">=</span><span class="n">run2</span>
<span class="o">./</span><span class="n">host</span><span class="o">.</span><span class="n">exe</span> <span class="n">vadd</span><span class="o">.</span><span class="n">hw</span><span class="o">.</span><span class="n">run2</span><span class="o">.</span><span class="n">xclbin</span>
</pre></div>
</div>
<p>A sample output from the run as below</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Buffer</span> <span class="n">Inputs</span> <span class="mi">2</span> <span class="n">MB</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span><span class="mi">3573</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">3</span><span class="p">]:</span><span class="mi">3577</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">4</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">5</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">6</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">9</span><span class="p">]:</span><span class="mi">3576</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">10</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">11</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">12</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">13</span><span class="p">]:</span><span class="mi">3574</span>
<span class="n">kernel</span><span class="p">[</span><span class="mi">14</span><span class="p">]:</span><span class="mi">3575</span>
<span class="n">Total</span> <span class="n">Kernel</span> <span class="n">execution</span> <span class="ow">in</span> <span class="mi">20</span> <span class="n">seconds</span><span class="p">:</span><span class="mi">53625</span>

<span class="n">Data</span> <span class="n">processed</span> <span class="ow">in</span> <span class="mi">20</span> <span class="n">seconds</span><span class="p">:</span> <span class="mi">4</span><span class="n">MB</span><span class="o">*</span><span class="n">total_kernel_executions</span><span class="p">:</span><span class="mi">214500</span> <span class="n">MB</span>

<span class="n">Data</span> <span class="n">processed</span><span class="o">/</span><span class="n">sec</span> <span class="p">(</span><span class="n">GBPs</span><span class="p">)</span><span class="o">=</span> <span class="mf">10.725</span> <span class="n">GBPs</span>
<span class="n">TEST</span> <span class="n">SUCCESS</span>
</pre></div>
</div>
<p>As you can see from a sample run above the number of kernel executions has been increased in host memory setup thus increasing the throughput of the application to 10.7 GBPs</p>
<p>Open the vitis_analyzer using the newly generated .run_summary file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vitis_analyzer</span> <span class="n">xrt</span><span class="o">.</span><span class="n">run_summary</span>
</pre></div>
</div>
<p>In the <strong>Kernel and Compute Units</strong> section you can see average CU execution times are now increased compared to the DDR-based run. Now CU takes more time as accessing the remote memory on the host machine is always slower than accessing on-chip memory on the FPGA card.  However, increasing CU time is not appearing as an overall negative result as the number of CU executions is increased for each CU. In a host memory-based application, the host CPU is not performing any data transfer operation. This can free up CPU cycles which can then otherwise used to increase the overall application performance. In this example, the free CPU cycles helped in processing more CU execution requests resulting in more accomplished data processing within the same period.</p>
<img alt="../../../../_images/hm_profile.JPG" class="align-center" src="../../../../_images/hm_profile.JPG" />
<p>Unlike DDR-based applications, you cannot see the <strong>Host Transfer</strong> section inside the Profile report. As there are no data transfers initiated by the host machine, this report is not populated.</p>
<p>You can review Application timeline as below</p>
<img alt="../../../../_images/at_hm.jpg" class="align-center" src="../../../../_images/at_hm.jpg" />
<p>Hovering the mouse on one of the data transfers shows the type of Data transfer is <strong>Host Memory Synchronization</strong>. This signifies the data transfer is merely a cache synchronization operation from the host operation perspective. As this cache invalidate/flush is very fast it has very little overhead on the host machine. The snapshot also shows under the <strong>Kernel Enqueues</strong> section there are now a greater number of rows (ROW0 to ROW9) signifying the host is now able to submit more kernel execution requests in parallel.</p>
</div>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h1>
<p>In summary, you have reviewed the following takeaways in this tutorial</p>
<ul class="simple">
<li><p>Easy migration from a DDR based application to a host memory-based application</p></li>
</ul>
<ol class="arabic simple">
<li><p>Kernel linking switch change</p></li>
<li><p>Host code change</p></li>
</ol>
<ul class="simple">
<li><p>Comparing and understanding Profile and Application timeline</p></li>
<li><p>A host memory-based paradigm can help to eliminate the data transfer burden from the host. In some usecases this might help to boost overall application performance.</p></li>
</ul>
<hr class="docutils" />
<p>Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at: <a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
<p>Copyright 2020–2022 Xilinx</p>
</div>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../07-using-hbm/README.html" class="btn btn-neutral float-left" title="Using HBM" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../09-using-ethernet-on-alveo/README.html" class="btn btn-neutral float-right" title="Using GT Kernel in Alveo with Vitis Flow" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2022, Xilinx, Inc..
      <span class="lastupdated">Last updated on May 16, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>