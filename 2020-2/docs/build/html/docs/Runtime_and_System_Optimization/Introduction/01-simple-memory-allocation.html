<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
  <title>Overview &mdash; Vitis™ Tutorials 2020.2 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../../index.html" class="icon icon-home"> Vitis™ Tutorials
            <img src="../../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2020.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">日本語版</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/master/docs-jp/README.html">Master</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started Pathway</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Getting_Started/Vitis/README.html">Vitis Flow 101 Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Getting_Started/Vitis_HLS/README.html">Vitis HLS Analysis and Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Hardware Accelerators</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Introduction/README.html">Introduction to Vitis Hardware Accelerators Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Design_Tutorials/02-bloom/README.html">Optimizing Accelerated FPGA Applications: Bloom Filter Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Design_Tutorials/01-convolution-tutorial/README.html">Accelerating Video Convolution Filtering Application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Design_Tutorials/03-rtl_stream_kernel_integration/README.html">Mixed Kernels Design Tutorial with AXI Stream and Vitis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Design_Tutorials/04-traveling-salesperson/README.html">The Travelling Salesman Problem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Design_Tutorials/05-bottom_up_rtl_kernel/README.html">Bottom-up RTL Kernel Flow with Vitis for Acceleration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Feature_Tutorials/01-rtl_kernel_workflow/README.html">Getting Started with RTL Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Feature_Tutorials/02-mixing-c-rtl-kernels/README.html">Mixing C++ and RTL Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Hardware_Accelerators/Feature_Tutorials/03-dataflow_debug_and_optimization/README.html">Vitis HLS Analysis and Optimization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Runtime and System Optimization</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Design_Tutorials/01-host-code-opt/README.html">Host Code Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Design_Tutorials/02-ivas-ml/README.html">IVAS ZCU104 ML Acceleration Reference Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Feature_Tutorials/01-mult-ddr-banks/README.html">Using Multiple DDR Banks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Feature_Tutorials/02-using-multiple-cu/README.html">Using Multiple Compute Units</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Feature_Tutorials/03-controlling-vivado-implementation/README.html">Controlling Vivado Implementation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Feature_Tutorials/04-using-hbm/README.html">Using HBM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Vitis Platform Creation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../Vitis_Platform_Creation/Introduction/01-Overview/README.html">Platform Creation Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Vitis_Platform_Creation/Introduction/02-Edge-AI-ZCU104/README.html">Vitis Custom Embedded Platform Creation Example on ZCU104</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Vitis_Platform_Creation/Introduction/03_Edge_VCK190/README.html">Versal Custom Platform Creation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/Vitis-Tutorials/">Main</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis™ Tutorials</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/Runtime_and_System_Optimization/Introduction/01-simple-memory-allocation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <table width="100%">
 <tr width="100%">
    <td align="center"><img src="https://raw.githubusercontent.com/Xilinx/Image-Collateral/main/xilinx-logo.png" width="30%"/><h1>2020.2 Vitis™ - Runtime and System Optimization<br/>Example 1: Simple Memory Allocation</h1>
    <a href="https://www.xilinx.com/products/design-tools/vitis.html">See Vitis™ Development Environment on xilinx.com</a>
    </td>
 </tr>
</table><div class="section" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h1>
<p>The FPGA image that we’ve loaded contains a very simple vector addition core.  It takes two buffers of
arbitrary length as inputs and produces a buffer of equal length as an output.  As the name implies, during
the process it adds them together.</p>
<p>Our code has not really been optimized to run well in an FPGA.  It’s mostly equivalent to directly putting the
algorithm in the listing below directly into the FPGA fabric.  This isn’t particularly efficient. the
implementation tools will do some optimization for us, but we can make it better. With this code, we can
process one addition operation on each tick of the clock but we’re still only processing one 32-bit output at
a time.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">vadd_sw</span><span class="p">(</span><span class="kt">uint32_t</span><span class="w"> </span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="w"> </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">size</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">       </span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>It’s very important to note that at this point there is no way this code will beat the processor.  The clock
in the FPGA fabric is significantly slower than the CPU clock.  This is expected, though - thinking back to
our earlier example, we’re only loading a single passenger into each car on the train.  We also have overhead
to pass the data over PCIe, set up DMA, etc. For the next few examples, we’ll look at how to efficiently
manage the buffers for our inputs and outputs to this function.  Only after that will we start to take
advantage of the acceleration we can get from the Alveo Data Center accelerator card.</p>
</div>
<div class="section" id="key-code">
<h1>Key Code<a class="headerlink" href="#key-code" title="Permalink to this heading">¶</a></h1>
<p>This example is the first time we’re going to actually run something on the FPGA, modest though it may be.  In order to run something on the card there are four things that we must do:</p>
<ol class="simple">
<li><p>Allocate and populate the buffers we?ll use to send and receive data from the card.</p></li>
<li><p>Transfer those buffers between the host memory space and the Alveo global memory.</p></li>
<li><p>Run the kernel to act on those buffers.</p></li>
<li><p>Transfer the results of the kernel operation back to the host memory space so that they can be accessed via the processor.</p></li>
</ol>
<p>As you can see, only one of those things actually takes place on the card. Memory management will make or break your application’s performance, so let’s start to take a look at that.</p>
<p>If you haven’t done acceleration work before, you may be tempted to jump in and just use normal calls to <code class="docutils literal notranslate"><span class="pre">malloc()</span></code> or <code class="docutils literal notranslate"><span class="pre">new</span></code> to allocate your memory.  In this example we’ll do just that, allocating a series of buffers to transfer between the host and the Alveo card.  We’ll allocate four buffers: two input buffers to add together, one output buffer for the Alveo to use, and an extra buffer for a software implementation of our <code class="docutils literal notranslate"><span class="pre">vadd</span></code> function.  This allows us to see something interesting: how we allocate memory for Alveo also impacts how efficiently the processor will run.</p>
<p>Buffers are allocated simply, as in the listing below.  In our case, <code class="docutils literal notranslate"><span class="pre">BUFSIZE</span></code> is 24 MiB, or 6 × 1024 × 1024 values of type <code class="docutils literal notranslate"><span class="pre">uint32_t</span></code>.  Any code not mentioned here is either identical or functionally equivalent to the previous examples.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">uint32_t</span><span class="o">*</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="k">new</span><span class="w"> </span><span class="kt">uint32_t</span><span class="p">[</span><span class="n">BUFSIZE</span><span class="p">];</span><span class="w"></span>
<span class="kt">uint32_t</span><span class="o">*</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="k">new</span><span class="w"> </span><span class="kt">uint32_t</span><span class="p">[</span><span class="n">BUFSIZE</span><span class="p">];</span><span class="w"></span>
<span class="kt">uint32_t</span><span class="o">*</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="k">new</span><span class="w"> </span><span class="kt">uint32_t</span><span class="p">[</span><span class="n">BUFSIZE</span><span class="p">];</span><span class="w"></span>
<span class="kt">uint32_t</span><span class="o">*</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="k">new</span><span class="w"> </span><span class="kt">uint32_t</span><span class="p">[</span><span class="n">BUFSIZE</span><span class="p">];</span><span class="w"></span>
</pre></div>
</div>
<p>This will allocate memory that is <strong>virtual</strong>, <strong>paged</strong>, and, most importantly, <strong>non-aligned</strong>.  In
particular it’s this last one that is going to cause some problems, as we’ll soon see.</p>
<p>Once we allocate the buffers and populate them with initial test vectors, the next acceleration step is to
send them down to the Alveo global memory.  We do that by creating OpenCL buffer objects using the flag
<code class="docutils literal notranslate"><span class="pre">CL_MEM_USE_HOST_PTR</span></code>.  This tells the API that rather than allocating its own buffer, we are providing our
own pointers. This isn’t necessarily bad, but because we haven’t taken care allocating our pointers it’s going
to hurt our performance.</p>
<p>The listing below contains the code mapping our allocated buffers to OpenCL buffer objects.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">cl</span><span class="o">::</span><span class="n">Memory</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inBufVec</span><span class="p">,</span><span class="w"> </span><span class="n">outBufVec</span><span class="p">;</span><span class="w"></span>
<span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="w"> </span><span class="nf">a_to_device</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"></span>
<span class="w">                       </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cl_mem_flags</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CL_MEM_READ_ONLY</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="w">                                                 </span><span class="n">CL_MEM_USE_HOST_PTR</span><span class="p">),</span><span class="w"></span>
<span class="w">                       </span><span class="n">BUFSIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="w"> </span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">),</span><span class="w"></span>
<span class="w">                       </span><span class="n">a</span><span class="p">,</span><span class="w"></span>
<span class="w">                       </span><span class="nb">NULL</span><span class="p">);</span><span class="w"></span>
<span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="w"> </span><span class="nf">b_to_device</span><span class="w"> </span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"></span>
<span class="w">                       </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cl_mem_flags</span><span class="p">(</span><span class="n">CL_MEM_READ_ONLY</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="w">                                                </span><span class="n">CL_MEM_USE_HOST_PTR</span><span class="p">),</span><span class="w"></span>
<span class="w">                       </span><span class="n">BUFSIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">),</span><span class="w"></span>
<span class="w">                       </span><span class="n">b</span><span class="p">,</span><span class="w"></span>
<span class="w">                       </span><span class="nb">NULL</span><span class="p">);</span><span class="w"></span>
<span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="w"> </span><span class="nf">c_from_device</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="w"></span>
<span class="w">                        </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cl_mem_flags</span><span class="o">&gt;</span><span class="p">(</span><span class="n">CL_MEM_WRITE_ONLY</span><span class="w"> </span><span class="o">|</span><span class="w"></span>
<span class="w">                                                  </span><span class="n">CL_MEM_USE_HOST_PTR</span><span class="p">),</span><span class="w"></span>
<span class="w">                        </span><span class="n">BUFSIZE</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">uint32_t</span><span class="p">),</span><span class="w"></span>
<span class="w">                        </span><span class="n">c</span><span class="p">,</span><span class="w"></span>
<span class="w">                        </span><span class="nb">NULL</span><span class="p">);</span><span class="w"></span>
<span class="n">inBufVec</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">a_to_device</span><span class="p">);</span><span class="w"></span>
<span class="n">inBufVec</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">b_to_device</span><span class="p">);</span><span class="w"></span>
<span class="n">outBufVec</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">c_from_device</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>What we’re doing here is allocating <code class="docutils literal notranslate"><span class="pre">cl::Buffer</span></code> objects, which are recognized by the API, and passing in
pointers <code class="docutils literal notranslate"><span class="pre">a</span></code>, <code class="docutils literal notranslate"><span class="pre">b</span></code>, and <code class="docutils literal notranslate"><span class="pre">c</span></code> from our previously-allocated buffers.  The additional flags <code class="docutils literal notranslate"><span class="pre">CL_MEM_READ_ONLY</span></code> and
<code class="docutils literal notranslate"><span class="pre">CL_MEM_WRITE_ONLY</span></code> specify to the runtime the visibility of these buffers from the perspective of the
kernel.  In other words, <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are written to the card by the host - to the kernel they are <strong>read
only</strong>.  Then, <code class="docutils literal notranslate"><span class="pre">c</span></code> is read back from the card to the host.  To the kernel it is <strong>write only</strong>.  We
additionally add these buffer objects to vectors so that we can transfer multiple buffers at once (note that
we’re essentially adding pointers to the vectors, not the data buffers themselves).</p>
<p>Next, we can transfer the input buffers down to the Alveo card:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">cl</span><span class="o">::</span><span class="n">Event</span><span class="w"> </span><span class="n">event_sp</span><span class="p">;</span><span class="w"></span>
<span class="n">q</span><span class="p">.</span><span class="n">enqueueMigrateMemObjects</span><span class="p">(</span><span class="n">inBufVec</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">event_sp</span><span class="p">);</span><span class="w"></span>
<span class="n">clWaitForEvents</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">cl_event</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">event_sp</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>In this code snippet the “main event” is the call to enqueueMigrateMemObjects() on line 108.  We pass in our
vector of buffers, the 0 indicates that this is a transfer from host to device, and we also pass in a
<code class="docutils literal notranslate"><span class="pre">cl::Event</span></code> object.</p>
<p>This is a good time to segue briefly into synchronization. When we enqueue the transfer we’re adding it to the
runtime’s ‘to-do list’, if you will, but not actually waiting for it to complete.  By registering a
<code class="docutils literal notranslate"><span class="pre">cl::Event</span></code> object, we can then decide to wait on that event at any point in the future.  In general this
isn’t a point where you would necessarily want to wait, but we’ve done this at various points throughout the
code to more easily instrument it to display the time taken for various operations.  This adds a small amount
of overhead to the application, but again, this is a learning exercise and not an example of optimizing for
maximum performance.</p>
<p>We now need to tell the runtime what to pass to our kernel, and we do that in the next listing.  Recall that
our argument list looked like this:</p>
<p><code class="docutils literal notranslate"><span class="pre">(uint32_t*a,</span> <span class="pre">uint32_t*b,</span> <span class="pre">uint32_t*c,</span> <span class="pre">uint32_t</span> <span class="pre">size)</span></code></p>
<p>In our case <code class="docutils literal notranslate"><span class="pre">a</span></code> is argument 0, <code class="docutils literal notranslate"><span class="pre">b</span></code> is argument 1, and so on.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">krnl</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">a_to_device</span><span class="p">);</span><span class="w"></span>
<span class="n">krnl</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">b_to_device</span><span class="p">);</span><span class="w"></span>
<span class="n">krnl</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">c_from_device</span><span class="p">);</span><span class="w"></span>
<span class="n">krnl</span><span class="p">.</span><span class="n">setArg</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">BUFSIZE</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Next, we add the kernel itself to the command queue so that it will begin executing.  Generally speaking, you
would enqueue the transfers and the kernel such that they’d execute back-to-back rather than synchronizing in
between.  The line of code that adds the execution of the kernel to the command queue is:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">q</span><span class="p">.</span><span class="n">enqueueTask</span><span class="p">(</span><span class="n">krnl</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">event_sp</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>If you don’t want to wait at this point you can again pass in <code class="docutils literal notranslate"><span class="pre">NULL</span></code> instead of a <code class="docutils literal notranslate"><span class="pre">cl::Event</span></code> object.</p>
<p>And, finally, once the kernel completes we want to transfer the memory back to the host so that we can access
the new values from the CPU.  This is done as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">q</span><span class="p">.</span><span class="n">enqueueMigrateMemObjects</span><span class="p">(</span><span class="n">outBufVec</span><span class="p">,</span><span class="w"> </span><span class="n">CL_MIGRATE_MEM_OBJECT_HOST</span><span class="p">,</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">event_sp</span><span class="p">);</span><span class="w"></span>
<span class="n">clWaitForEvents</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">cl_event</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">event_sp</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>In this instance we do want to wait for synchronization.  This is important; recall that when we call these
enqueue functions, we’re placing entries onto the command queue in a <strong>non-blocking</strong> manner.  If we then
attempt to access the buffer immediately after enqueuing the transfer, it have finished reading back in.</p>
<p>Excluding the FPGA configuration from example 0, the new additions in order to run the kernel are:</p>
<ol class="simple">
<li><p>Allocate buffers in the normal way.  We’ll soon see that there are better ways of doing this, but this is the way many people experimenting with acceleration might do it their first time.</p></li>
<li><p>Map the allocated buffers to cl::Buffer objects.</p></li>
<li><p>Enqueue the migration of the input buffers (a and b) to Alveo device global memory.</p></li>
<li><p>Set the kernel arguments, both buffers and scalar values.</p></li>
<li><p>Run the kernel.</p></li>
<li><p>Read the results of the kernel back into CPU host memory, synchronizing on the completion of the read.</p></li>
</ol>
<p>Only one synchronization is needed were this a real application.  As previously, mentioned we’re using several
to better report on the timing of various operations in the workflow.</p>
</div>
<div class="section" id="running-the-application">
<h1>Running the Application<a class="headerlink" href="#running-the-application" title="Permalink to this heading">¶</a></h1>
<p>With the XRT initialized, run the application by running the following command from the build directory.</p>
<p><code class="docutils literal notranslate"><span class="pre">./01_simple_malloc</span> <span class="pre">alveo_examples</span></code></p>
<p>The program will output a message similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>-- Example 1: Vector Add with Malloc() --

Loading XCLBin to program the Alveo board:

Found Platform
Platform Name: Xilinx
XCLBINFile Name: alveo_examples
INFO: Importing ./alveo_examples.xclbin
Loading: ./alveo_examples.xclbin
Running kernel test with malloc()ed buffers
WARNING: unaligned host pointer 0x154f7909e010 detected, this leads to extra memcpy
WARNING: unaligned host pointer 0x154f7789d010 detected, this leads to extra memcpy
WARNING: unaligned host pointer 0x154f7609c010 detected, this leads to extra memcpy

Simple malloc vadd example complete!

--------------- Key execution times ---------------
OpenCL Initialization:              247.371 ms
Allocating memory buffer:           0.030 ms
Populating buffer inputs:           47.955 ms
Software VADD run:                  35.706 ms
Map host buffers to OpenCL buffers: 64.656 ms
Memory object migration enqueue:    24.829 ms
Set kernel arguments:               0.009 ms
OCL Enqueue task:                   0.064 ms
Wait for kernel to complete:        92.118 ms
Read back computation results:      24.887 ms
</pre></div>
</div>
<p>Note that we have some warnings about unaligned host pointers.  Because we didn’t take care with our
allocation, none of our buffers that we’re transferring to or from the Alveo card are aligned to the 4 KiB
boundaries needed by the Alveo DMA engine.  Because of this, we need to copy the buffer contents so they’re
aligned before transfer, and that operation is quite expensive.</p>
<p>From this point on in our examples, let’s keep a close eye on these numbers.  While there will be some
variability on the latency run-to-run, generally speaking we are looking for deltas in each particular area.
For now let’s establish a baseline:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Operation</th>
<th align="center">Example 1</th>
</tr>
</thead>
<tbody>
<tr>
<td>OCL Initialization</td>
<td align="center">247.371 ms</td>
</tr>
<tr>
<td>Buffer Allocation</td>
<td align="center">30 &micro;s</td>
</tr>
<tr>
<td>Buffer Population</td>
<td align="center">47.955 ms</td>
</tr>
<tr>
<td>Software VADD</td>
<td align="center">35.706 ms</td>
</tr>
<tr>
<td>Buffer Mapping</td>
<td align="center">64.656 ms</td>
</tr>
<tr>
<td>Write Buffers Out</td>
<td align="center">24.829 ms</td>
</tr>
<tr>
<td>Set Kernel Args</td>
<td align="center">9 &micro;s</td>
</tr>
<tr>
<td>Kernel Runtime</td>
<td align="center">92.118 ms</td>
</tr>
<tr>
<td>Read Buffer In</td>
<td align="center">24.887 ms</td>
</tr>
<tr>
<td>&Delta;Alveo&rarr;CPU</td>
<td align="center">-418.228 ms</td>
</tr>
<tr>
<td>&Delta;Alveo&rarr;CPU (algorithm only)</td>
<td align="center">-170.857 ms</td>
</tr>
</tbody>
</table></div>
<div class="section" id="extra-exercises">
<h1>Extra Exercises<a class="headerlink" href="#extra-exercises" title="Permalink to this heading">¶</a></h1>
<p>Some things to try to build on this experiment:</p>
<ul class="simple">
<li><p>Vary the size of the buffers allocated.  Can you derive an approximate relationship between buffer size and
the timing for individual operations? Do they all scale at the same rate?</p></li>
<li><p>If you remove synchronization between each step, what is the quantitative effect on the runtime?</p></li>
<li><p>What happens if you remove the synchronization after the final buffer copy from Alveo back to the host?</p></li>
</ul>
</div>
<div class="section" id="key-takeaways">
<h1>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permalink to this heading">¶</a></h1>
<ul class="simple">
<li><p>Once again we have to pay our FPGA configuration “tax”.  We will need to save at least 250 ms over the CPU
to make up for it.  Note that our trivial example will never be at the CPU if we’re just looking at
processing a single buffer!</p></li>
<li><p>Simply-allocated memory isn’t a good candidate for passing to accelerators, as we’ll incur a memory copy to
compensate.  We’ll investigate the impact this has in subsequent examples.</p></li>
<li><p>OpenCL works on command queues.  It’s up to the developer how and when to synchronize, but care must be
taken when reading buffers back in from the Alveo global memory to ensure synchronization before the CPU
accesses the data in the buffer.</p></li>
</ul>
<p>Read <a class="reference internal" href="02-aligned-memory-allocation.html"><span class="doc">Example 2: Aligned Memory Allocation</span></a></p>
<p align="center"><sup>Copyright&copy; 2019-2021 Xilinx</sup></p></div>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019-2022, Xilinx, Inc. Xilinx is now a part of AMD.
      <span class="lastupdated">Last updated on August 5, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>